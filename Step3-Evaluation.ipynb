{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1a25959-2431-4178-8865-c26c7fcb2a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup inference and evaluation parameters\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "GPU_ID=\"0,1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=GPU_ID\n",
    "LoadBaseOutput=False\n",
    "Customer=1\n",
    "CustomPrompt=1\n",
    "BaseModelName='goliath-120b-AWQ'\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "import Libs.DataSynthesisLib as DataSynthesisLib\n",
    "Config=DataSynthesisLib.GetConfig('Config/Config.py')\n",
    "\n",
    "ModelFT='./tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05'\n",
    "#ModelBase='unsloth/Meta-Llama-3.1-8B-Instruct'\n",
    "ModelBase='TheBloke/'+BaseModelName\n",
    "\n",
    "\n",
    "AssistantPromptElementsRemoval=2\n",
    "EvalLLM='microsoft/phi-4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851eca19-e15c-4621-a9b5-03fd195bd607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load appropriate evaluation dataset\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "if Customer==1:\n",
    "  with open('Data/CustomerComments_Evaluation_Clean.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "elif Customer==0:\n",
    "  with open('Data/ClouderaComments_Evaluation_Clean.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9d6ca99-405d-4a7b-8d49-62f92918d507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#Setup questions for LLM-as-a-judge, collect prompts for inference, and customer or Cloudera comments to be used later for evaluation\n",
    "import vllm\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(ModelFT)\n",
    "\n",
    "System='You are a helpful assistant.'\n",
    "\n",
    "\n",
    "FixedQuestions={}\n",
    "if Customer==1:\n",
    "  FixedQuestions[1]=\"Does this comment discuss any technical information? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[2]=\"Does this comment relate to a customer complaint? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[3]=\"Customer complaint temperature or a frustration level (if there is a complain give a Score 1-4, lowest=1, highest=4. If there is no complain give a score of 0)\"\n",
    "  FixedQuestions[4]=\"Score the severity of the issue based on comment content (SCORE 1-4, lowest=1, highest=4)\"\n",
    "  FixedQuestions[5]=\"Score the urgency of the issue based on the comment content (SCORE 1-4, lowest=1, highest=4)\"\n",
    "  FixedQuestions[6]=\"Is this a request from a customer for an update? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[7]=\"Is there a strictly explicit and NOT an implied request from a customer for a call, meeting or a screenshare (zoom/webex/teams etc.)? Do not answer yes unless wording explicitly asks for a call. (BOOL)\"\n",
    "  FixedQuestions[8]=\"Did the customer request an escalation? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[9]=\"Did the customer request a priority change?  To what level? (If there is a priority change give score 1 to indicate highest priority (indicated by S1) and 4  to indicate the lowest priority (Indicated by 4). If there is no priority change give a score of 0).\"\n",
    "  FixedQuestions[10]=\"Did the customer request a transfer to another COE? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[11]=\"Did the customer request to speak to a manager or supervisor? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[12]=\"Did the customer request a SME or expert? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[13]=\"Does this comment discuss a bug in Cloudera software? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[14]=\"Does the comment include a non-Cloudera Apache JIRA link? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[15]=\"Does the comment have a link to Cloudera Documentation or Community article? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[16]=\"Does the comment have any other type of hyperlink? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[17]=\"Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]\"\n",
    "elif Customer==0:\n",
    "  FixedQuestions[1]=\"Does this comment discuss any technical information? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[2]=\"Score the severity of the issue based on comment content (SCORE 1-4, lowest=1, highest=4)\"\n",
    "  FixedQuestions[3]=\"Score the urgency of the issue based on the comment content (SCORE 1-4, lowest=1, highest=4)\"\n",
    "  FixedQuestions[4]=\"Does the comment have a proposed solution? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[5]=\"Does the comment have a proposed workaround?  (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[6]=\"Does the comment have a request for an action from the customer?  (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[7]=\"Does this comment discuss a bug in Cloudera software? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[8]=\"Does the comment include a non-Cloudera Apache JIRA link? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[9]=\"Does the comment have a link to Cloudera Documentation or Community article? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[10]=\"Does the comment have any other type of hyperlink? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[11]=\"Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT] \"\n",
    "\n",
    "\n",
    "Questions=[]\n",
    "Comment=[]\n",
    "BaselineSolutions=[]\n",
    "for i in data:\n",
    "  if Customer==1:\n",
    "      \n",
    "    if CustomPrompt==1:\n",
    "\n",
    "      Questions.append(i['Prompt'])\n",
    "      Comment.append(i['Comment'])\n",
    "\n",
    "  elif Customer==0:\n",
    "\n",
    "    CommentText=\"\"\"You are given a Cloudera support team comment, 11 questions referring to the comment, and the possible values for each question in parenthesis following the question. Here is the Cloudera comment:\n",
    "\n",
    "\"\"\"\n",
    "    PreppendClouderaQuestions=\"\"\"\n",
    "\n",
    "Here are the 11 questions:\n",
    "1. Does this comment discuss any technical information? (answer 0 for no, 1 for yes)\n",
    "2. Score the severity of the issue based on comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
    "3. Score the urgency of the issue based on the comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
    "4. Does the comment have a proposed solution? (answer 0 for no, 1 for yes)\n",
    "5. Does the comment have a proposed workaround?  (answer 0 for no, 1 for yes)\n",
    "6. Does the comment have a request for an action from the customer?  (answer 0 for no, 1 for yes)\n",
    "7. Does this comment discuss a bug in Cloudera software? (answer 0 for no, 1 for yes)\n",
    "8. Does the comment include a non-Cloudera Apache JIRA link (e.g. a Apache JIRA link with issues.apach.org domain name)? (answer 0 for no, 1 for yes)\n",
    "9. Does the comment have a link to Cloudera Documentation or Community article? (answer 0 for no, 1 for yes)\n",
    "10. Does the comment have any other type of hyperlink? (answer 0 for no, 1 for yes)\n",
    "11. Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]    \n",
    "Generate each answer in a new line using the the format \"Number. answer\" (e.g. 1. 0)\n",
    "\"\"\"\n",
    "    if CustomPrompt==1:\n",
    "      Questions.append(i['Prompt'])\n",
    "      Comment.append(i['Comment'])\n",
    "\n",
    "\n",
    "print(len(Comment))\n",
    "print(len(Questions))\n",
    "lens=[len(i) for i in Questions ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c9bc38-2e58-4e5e-9cc4-c1ca44dfe7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a customer comment, 17 questions referring to the comment, and the possible values for each question in parenthesis following the question. Here is the customer comment:\n",
      "\n",
      "Hi Cloudera Support,\\n\\nI need urgent assistance with account authorization for our CDP production environment. I'm getting 'Authorization Error: Token Invalid' messages when trying to access critical resources, and none of our automation tools are working. This seems to have started after our scheduled maintenance window last night. We have multiple production jobs failing because of this.\\n\\nPlease help resolve this as soon as possible.\\n\\nBest regards,\\nMark\n",
      "\n",
      "Here are the 17 questions:\n",
      "1. Does this comment discuss any technical information? (answer 0 for no, 1 for yes)\n",
      "2. Does this comment relate to a customer complaint? (answer 0 for no, 1 for yes)\n",
      "3. Customer complaint temperature or a frustration level (if there is a complain give 1 for lowest, 4 for highest and 2,3 for in between. If there is no complain give a score of 0).\n",
      "4. Score the severity of the issue based on comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "5. Score the urgency of the issue based on the comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "6. Is this a request from a customer for an update? (answer 0 for no, 1 for yes)\n",
      "7. Is there a strictly explicit and NOT an implied request from a customer for a call, meeting or a screenshare (zoom/webex/teams etc.)? Do not answer yes unless wording explicitly asks for a call. ((BOOL:0/1)\n",
      "8. Did the customer request an escalation? (answer 0 for no, 1 for yes)\n",
      "9. Did the customer request a priority change?  To what level? (If there is a priority change give score 1 to indicate highest priority (indicated by S1) and 4  to indicate the lowest priority (Indicated by S4). If there is no priority change give a score of 0).\n",
      "10. Did the customer request a transfer to another Customer Operations Engineer? (answer 0 for no, 1 for yes)\n",
      "11. Did the customer request to speak to a manager or supervisor? (answer 0 for no, 1 for yes)\n",
      "12. Did the customer request a Subject Matter Expert or expert? (answer 0 for no, 1 for yes)\n",
      "13. Does this comment discuss a bug in Cloudera software? (answer 0 for no, 1 for yes)\n",
      "14. Does the comment include a non-Cloudera Apache JIRA link (e.g. a Apache JIRA link with issues.apach.org domain name)? (answer 0 for no, 1 for yes)\n",
      "15. Does the comment have a link to Cloudera Documentation or Community article? (answer 0 for no, 1 for yes)\n",
      "16. Does the comment have any other type of hyperlink? (answer 0 for no, 1 for yes)\n",
      "17. Summarize the case comment condensing itas much as possible but without losing important technical details. Omit including any meeting invite information. (TEXT)\n",
      "Generate each answer in a new line using the the format \"Number. answer\" (e.g. 1. 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Typical example on how the LLM is prompted\n",
    "print(Questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c1251c9-4b6c-4112-944c-6cd485af443b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TheBloke/goliath-120b-AWQ\n",
      "INFO 03-24 17:49:09 awq_marlin.py:97] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 03-24 17:49:09 config.py:887] Defaulting to use mp for distributed inference\n",
      "WARNING 03-24 17:49:09 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 03-24 17:49:09 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='TheBloke/goliath-120b-AWQ', speculative_config=None, tokenizer='TheBloke/goliath-120b-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=TheBloke/goliath-120b-AWQ, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 03-24 17:49:10 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 03-24 17:49:10 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3046)\u001b[0;0m INFO 03-24 17:49:19 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "INFO 03-24 17:49:19 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3046)\u001b[0;0m INFO 03-24 17:49:19 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 03-24 17:49:19 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3046)\u001b[0;0m INFO 03-24 17:49:19 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3046)\u001b[0;0m INFO 03-24 17:49:19 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/cdsw/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 03-24 17:49:19 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/cdsw/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 03-24 17:49:19 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fa760f0fcd0>, local_subscribe_port=52239, remote_subscribe_port=None)\n",
      "INFO 03-24 17:49:19 model_runner.py:1060] Starting to load model TheBloke/goliath-120b-AWQ...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3046)\u001b[0;0m INFO 03-24 17:49:19 model_runner.py:1060] Starting to load model TheBloke/goliath-120b-AWQ...\n",
      "INFO 03-24 17:49:20 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3046)\u001b[0;0m INFO 03-24 17:49:20 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:01<00:09,  1.53s/it]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:03<00:07,  1.56s/it]\n",
      "Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:04<00:06,  1.52s/it]\n",
      "Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:06<00:04,  1.49s/it]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:06<00:02,  1.10s/it]\n",
      "Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:07<00:01,  1.20s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:09<00:00,  1.32s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:09<00:00,  1.34s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 17:49:33 model_runner.py:1071] Loading model weights took 28.9056 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3046)\u001b[0;0m INFO 03-24 17:49:33 model_runner.py:1071] Loading model weights took 28.9056 GB\n",
      "INFO 03-24 17:49:41 distributed_gpu_executor.py:57] # GPU blocks: 3370, # CPU blocks: 0\n",
      "INFO 03-24 17:49:41 distributed_gpu_executor.py:61] Maximum concurrency for 4000 tokens per request: 13.48x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 500/500 [13:46<00:00,  1.65s/it, est. speed input: 508.30 toks/s, output: 1.01 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 18:03:31 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3046)\u001b[0;0m INFO 03-24 18:03:31 multiproc_worker_utils.py:242] Worker exiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W324 18:03:32.399122799 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n"
     ]
    }
   ],
   "source": [
    "if LoadBaseOutput==False:\n",
    "  #Base model parameters setup and inference \n",
    "  from torch.multiprocessing import Pool, Process, set_start_method\n",
    "  try:\n",
    "     set_start_method('spawn')\n",
    "  except RuntimeError:\n",
    "    pass\n",
    "  os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "  ConfigS=Config[\"ConfigSolution\"].copy()\n",
    "  ConfigS[\"gpu-id\"]=GPU_ID\n",
    "  ConfigS[\"tensor_parallel_size\"]=\"2\"\n",
    "  #ConfigS[\"tensor_parallel_size\"]=\"1\"\n",
    "\n",
    "  ConfigS[\"system\"]=System\n",
    "  ConfigS[\"use-grammar\"]=\"False\"\n",
    "  ConfigS[\"AssistantPrompt\"]=\"\"\n",
    "\n",
    "  ConfigS[\"max_tokens\"]=\"8000\"\n",
    "  ConfigS[\"temperature\"]=\"0.1\"\n",
    "  ConfigS[\"max_num_seqs\"]=\"2\"\n",
    "\n",
    "  ConfigS[\"max_num_seqs\"]=\"16\"\n",
    "  ConfigS[\"max_model_len\"]=\"32000\"\n",
    "  ConfigS[\"max_num_batched_tokens\"]=\"32000\"\n",
    " \n",
    "  ConfigS[\"max_model_len\"]=\"4000\"\n",
    "  ConfigS[\"max_num_batched_tokens\"]=\"4000\"\n",
    "\n",
    "\n",
    "  ConfigS[\"AssistantPromptElementsRemoval\"]=\"1\"\n",
    "  ConfigS[\"llm-path\"]=ModelBase\n",
    "  ConfigS[\"tokenizer-path\"]=ModelBase\n",
    "\n",
    "  ConfigS[\"tokenizer-file-gguf\"]=\"\"\n",
    "  ConfigS[\"temperature\"]=\"0\"\n",
    "  if \"mistral\" in ModelBase:\n",
    "    ConfigS[\"GenerateChatTemplate\"]=\"GenerateChatWithoutSystemWithAssistant\"\n",
    "  else:\n",
    "    ConfigS[\"GenerateChatTemplate\"]=\"GenerateChatWithSystem\"\n",
    "\n",
    "  ConfigS[\"UseOutlines\"]=\"False\"\n",
    "  ConfigS[\"AssistantPromptElementsRemoval\"]=\"1\"\n",
    "  ConfigS[\"tokenizer-file-gguf\"]=\"\"\n",
    "  if 'gptq' in ModelFT:\n",
    "    ConfigS[\"quantization\"]=\"gptq\"\n",
    "  ConfigS[\"gpu_memory_utilization\"]=\"0.99\"\n",
    "  ConfigS['chat_template']=\"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif true == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\\\n\\\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\\\'t know the answer to a question, please don\\\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\\\n' + content.strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\"\n",
    "\n",
    "\n",
    "  (SolutionsBase, SolutionsBaseLogging)=DataSynthesisLib.GetSolutions(ConfigS, Questions, ConfigS[\"system\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29036f3b-da75-4cc8-ae88-49b436d851ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LoadBaseOutput==False:\n",
    "  import  pickle\n",
    "  if Customer==1:\n",
    "    with open('Data/Eval_Customer_Questions_Solutions_'+BaseModelName+'.pickle', 'wb') as handle:\n",
    "      pickle.dump({\"Q\":Questions, \"S\":SolutionsBase}, handle)\n",
    "  elif Customer==0:\n",
    "    with open('Data/Eval_Cloudera_Questions_Solutions_'+BaseModelName+'.pickle', 'wb') as handle:\n",
    "      pickle.dump({\"Q\":Questions, \"S\":SolutionsBase}, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfe3974a-8a84-40df-8ce5-435d036f6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LoadBaseOutput==True:\n",
    "\n",
    "  import  pickle\n",
    "  if Customer==1:\n",
    "    with open('Data/Eval_Customer_Questions_Solutions_'+BaseModelName+'.pickle', 'rb') as handle:\n",
    "      Output=pickle.load( handle)\n",
    "  elif Customer==0:\n",
    "    with open('Data/Eval_Cloudera_Questions_Solutions_'+BaseModelName+'.pickle', 'rb') as handle:\n",
    "      Output=pickle.load( handle)\n",
    "\n",
    "\n",
    "  SolutionsBase=Output[\"S\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b73bca93-f7f2-4236-972d-8a1d343e0936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '\\n\\n1. 1. 1\\n2. 2. 0\\n3. 3. 0\\n4. 4. 2\\n5. 5. 3\\n6. 6. 0\\n7. 7. 1\\n8. 8. 0\\n9. 9. 0\\n10. 10. 0\\n11. 11. 0\\n12. 12. 0\\n13. 13. 0\\n14. 14. 0\\n15. 15. 0\\n16. 16. 0\\n17. 17. Request for a meeting to discuss Ranger policies implementation and role-based access control.',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '\\n\\n1. 0. 0\\n2. 1. 0\\n3. 1. 0\\n4. 1. 2\\n5. 1. 4\\n6. 1. 0\\n7. 1. 0\\n8. 1. 0\\n9. 1. 0\\n10. 1. 0\\n11. 1. 0\\n12. 1. 0\\n13. 1. 0\\n14. 1. 0\\n15. 1. 0\\n16. 1. 0\\n17. 1. Customer requests account cancellation and subscription termination, seeks guidance on cancellation process, and asks for required documentation or confirmation.',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Output[\"S\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4f97e1c-98b6-461a-b157-48888cf06427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05\n",
      "INFO 03-24 17:34:38 config.py:887] Defaulting to use mp for distributed inference\n",
      "WARNING 03-24 17:34:38 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 03-24 17:34:38 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='./tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05', speculative_config=None, tokenizer='./tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=./tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 03-24 17:34:39 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 03-24 17:34:39 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=2756)\u001b[0;0m INFO 03-24 17:34:47 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "INFO 03-24 17:34:48 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2756)\u001b[0;0m INFO 03-24 17:34:48 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 03-24 17:34:48 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2756)\u001b[0;0m INFO 03-24 17:34:48 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 03-24 17:34:48 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/cdsw/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2756)\u001b[0;0m INFO 03-24 17:34:48 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/cdsw/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 03-24 17:34:48 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f6865ae3100>, local_subscribe_port=38527, remote_subscribe_port=None)\n",
      "INFO 03-24 17:34:48 model_runner.py:1060] Starting to load model ./tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2756)\u001b[0;0m INFO 03-24 17:34:48 model_runner.py:1060] Starting to load model ./tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.85it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.92it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:03,  3.27s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  4.42s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.42s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=2756)\u001b[0;0m INFO 03-24 17:35:02 model_runner.py:1071] Loading model weights took 7.5122 GB\n",
      "INFO 03-24 17:35:02 model_runner.py:1071] Loading model weights took 7.5122 GB\n",
      "INFO 03-24 17:35:10 distributed_gpu_executor.py:57] # GPU blocks: 21256, # CPU blocks: 0\n",
      "INFO 03-24 17:35:10 distributed_gpu_executor.py:61] Maximum concurrency for 32000 tokens per request: 10.63x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 500/500 [01:56<00:00,  4.29it/s, est. speed input: 3248.07 toks/s, output: 529.82 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 17:37:10 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2756)\u001b[0;0m INFO 03-24 17:37:10 multiproc_worker_utils.py:242] Worker exiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W324 17:37:11.078179900 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n"
     ]
    }
   ],
   "source": [
    "#Finetuned model parameters setup and inference \n",
    "\n",
    "from torch.multiprocessing import Pool, Process, set_start_method\n",
    "try:\n",
    "     set_start_method('spawn')\n",
    "except RuntimeError:\n",
    "    pass\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "ConfigS=Config[\"ConfigSolution\"].copy()\n",
    "ConfigS[\"gpu-id\"]=GPU_ID\n",
    "ConfigS[\"tensor_parallel_size\"]=\"2\"\n",
    "#ConfigS[\"tensor_parallel_size\"]=\"1\"\n",
    "\n",
    "ConfigS[\"system\"]=System\n",
    "ConfigS[\"use-grammar\"]=\"False\"\n",
    "ConfigS[\"AssistantPrompt\"]=\"\"\n",
    "\n",
    "ConfigS[\"max_tokens\"]=\"3000\"\n",
    "ConfigS[\"temperature\"]=\"0.1\"\n",
    "ConfigS[\"max_num_seqs\"]=\"2\"\n",
    "\n",
    "ConfigS[\"max_num_seqs\"]=\"16\"\n",
    "ConfigS[\"max_model_len\"]=\"32000\"\n",
    "ConfigS[\"max_num_batched_tokens\"]=\"32000\"\n",
    "\n",
    "\n",
    "ConfigS[\"AssistantPromptElementsRemoval\"]=\"1\"\n",
    "ConfigS[\"llm-path\"]=ModelFT\n",
    "ConfigS[\"tokenizer-path\"]=ModelFT\n",
    "\n",
    "ConfigS[\"tokenizer-file-gguf\"]=\"\"\n",
    "ConfigS[\"temperature\"]=\"0\"\n",
    "if \"mistral\" in ModelFT:\n",
    "\n",
    "  ConfigS[\"GenerateChatTemplate\"]=\"GenerateChatWithoutSystemWithAssistant\"\n",
    "else:\n",
    "  ConfigS[\"GenerateChatTemplate\"]=\"GenerateChatWithSystem\"\n",
    "\n",
    "ConfigS[\"UseOutlines\"]=\"False\"\n",
    "ConfigS[\"AssistantPromptElementsRemoval\"]=\"1\"\n",
    "ConfigS[\"tokenizer-file-gguf\"]=\"\"\n",
    "if 'gptq' in ModelFT:\n",
    "  ConfigS[\"quantization\"]=\"gptq\"\n",
    "ConfigS[\"gpu_memory_utilization\"]=\"0.7\"\n",
    "\n",
    "\n",
    "(Solutions, SolutionsLogging)=DataSynthesisLib.GetSolutions(ConfigS, Questions, ConfigS[\"system\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ecbbdae-d9c1-4b0b-808e-78c67e620a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a customer comment, 17 questions referring to the comment, and the possible values for each question in parenthesis following the question. Here is the customer comment:\n",
      "\n",
      "Hello Cloudera Support, we've been experiencing some issues with our current Silver Support plan and would like to understand the benefits of upgrading to Gold Support. Our team needs faster response times for critical issues, and we're wondering if Gold Support would provide 24/7 coverage. Also, what's the typical timeline for implementing this change? Looking forward to your response.\n",
      "\n",
      "Here are the 17 questions:\n",
      "1. Does this comment discuss any technical information? (answer 0 for no, 1 for yes)\n",
      "2. Does this comment relate to a customer complaint? (answer 0 for no, 1 for yes)\n",
      "3. Customer complaint temperature or a frustration level (if there is a complain give 1 for lowest, 4 for highest and 2,3 for in between. If there is no complain give a score of 0).\n",
      "4. Score the severity of the issue based on comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "5. Score the urgency of the issue based on the comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "6. Is this a request from a customer for an update? (answer 0 for no, 1 for yes)\n",
      "7. Is there a strictly explicit and NOT an implied request from a customer for a call, meeting or a screenshare (zoom/webex/teams etc.)? Do not answer yes unless wording explicitly asks for a call. ((BOOL:0/1)\n",
      "8. Did the customer request an escalation? (answer 0 for no, 1 for yes)\n",
      "9. Did the customer request a priority change?  To what level? (If there is a priority change give score 1 to indicate highest priority (indicated by S1) and 4  to indicate the lowest priority (Indicated by S4). If there is no priority change give a score of 0).\n",
      "10. Did the customer request a transfer to another Customer Operations Engineer? (answer 0 for no, 1 for yes)\n",
      "11. Did the customer request to speak to a manager or supervisor? (answer 0 for no, 1 for yes)\n",
      "12. Did the customer request a Subject Matter Expert or expert? (answer 0 for no, 1 for yes)\n",
      "13. Does this comment discuss a bug in Cloudera software? (answer 0 for no, 1 for yes)\n",
      "14. Does the comment include a non-Cloudera Apache JIRA link (e.g. a Apache JIRA link with issues.apach.org domain name)? (answer 0 for no, 1 for yes)\n",
      "15. Does the comment have a link to Cloudera Documentation or Community article? (answer 0 for no, 1 for yes)\n",
      "16. Does the comment have any other type of hyperlink? (answer 0 for no, 1 for yes)\n",
      "17. Summarize the case comment condensing itas much as possible but without losing important technical details. Omit including any meeting invite information. (TEXT)\n",
      "Generate each answer in a new line using the the format \"Number. answer\" (e.g. 1. 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Typical example on how the LLM is prompted\n",
    "print(Questions[43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "570eadbc-ae16-4e2b-9dfa-d8ebef4603ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QuestionsBase' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mQuestionsBase\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'QuestionsBase' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c78a33c1-f752-4eae-b75a-5f5a328895bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1. 1\n",
      "2. 0\n",
      "3. 0\n",
      "4. 2\n",
      "5. 2\n",
      "6. 1\n",
      "7. 0\n",
      "8. 0\n",
      "9. 0\n",
      "10. 0\n",
      "11. 0\n",
      "12. 0\n",
      "13. 0\n",
      "14. 0\n",
      "15. 0\n",
      "16. 0\n",
      "17. Customer inquiring about benefits of upgrading from Silver to Gold Support, specifically regarding response times, 24/7 coverage, and implementation timeline.\n"
     ]
    }
   ],
   "source": [
    "#Example finetuned model output\n",
    "print(Solutions[43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b361ef07-647b-45ea-b16e-4c596b11df19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Example finetuned model output\n",
    "print(SolutionsBase[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3d33f85-fa4e-4fdc-908a-82978556328d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Does this comment discuss any technical information? (BOOL: 0 for no, 1 for yes)',\n",
       " 2: 'Does this comment relate to a customer complaint? (BOOL: 0 for no, 1 for yes)',\n",
       " 3: 'Customer complaint temperature or a frustration level (if there is a complain give a Score 1-4, lowest=1, highest=4. If there is no complain give a score of 0)',\n",
       " 4: 'Score the severity of the issue based on comment content (SCORE 1-4, lowest=1, highest=4)',\n",
       " 5: 'Score the urgency of the issue based on the comment content (SCORE 1-4, lowest=1, highest=4)',\n",
       " 6: 'Is this a request from a customer for an update? (BOOL: 0 for no, 1 for yes)',\n",
       " 7: 'Is there a strictly explicit and NOT an implied request from a customer for a call, meeting or a screenshare (zoom/webex/teams etc.)? Do not answer yes unless wording explicitly asks for a call. (BOOL)',\n",
       " 8: 'Did the customer request an escalation? (BOOL: 0 for no, 1 for yes)',\n",
       " 9: 'Did the customer request a priority change?  To what level? (If there is a priority change give score 1 to indicate highest priority (indicated by S1) and 4  to indicate the lowest priority (Indicated by 4). If there is no priority change give a score of 0).',\n",
       " 10: 'Did the customer request a transfer to another COE? (BOOL: 0 for no, 1 for yes)',\n",
       " 11: 'Did the customer request to speak to a manager or supervisor? (BOOL: 0 for no, 1 for yes)',\n",
       " 12: 'Did the customer request a SME or expert? (BOOL: 0 for no, 1 for yes)',\n",
       " 13: 'Does this comment discuss a bug in Cloudera software? (BOOL: 0 for no, 1 for yes)',\n",
       " 14: 'Does the comment include a non-Cloudera Apache JIRA link? (BOOL: 0 for no, 1 for yes)',\n",
       " 15: 'Does the comment have a link to Cloudera Documentation or Community article? (BOOL: 0 for no, 1 for yes)',\n",
       " 16: 'Does the comment have any other type of hyperlink? (BOOL: 0 for no, 1 for yes)',\n",
       " 17: 'Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Individual questions for evaluating each question using LLM-as-a-judge\n",
    "FixedQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccb086ce-ca2f-42d3-97d4-8fa3b526c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse LLM output and extract analytics and summary\n",
    "def FormatOutput(Solutions,StartLine=3,ExpectedQuestions=17):\n",
    "  CorrectFormat=0\n",
    "  Output=[]\n",
    "  Correct=[]\n",
    "\n",
    "  for d in Solutions:\n",
    "    result=d.split('\\n')\n",
    "    QuestionsAnswers={}\n",
    "    count=0\n",
    "    CorrectFormat=0\n",
    "      \n",
    "    for j in result[StartLine-1:]:\n",
    "      FoundNumber=0\n",
    "      count+=1\n",
    "      context=\"\"\n",
    "      number=j.split('.')[0]\n",
    "      if number.isdigit():\n",
    "          number=int(number)\n",
    "          if number==count:\n",
    "              FoundNumber=1\n",
    "      tmp=j.split(' ')\n",
    "      if len(tmp)>=2:\n",
    "         if count<ExpectedQuestions:\n",
    "             if tmp[1].isdigit():\n",
    "                 context=int(tmp[1])\n",
    "                 if FoundNumber==1:\n",
    "                    CorrectFormat+=1\n",
    "                    QuestionsAnswers[number]=context\n",
    "         else:\n",
    "             context=' '.join(tmp[1:])\n",
    "             if FoundNumber==1:\n",
    "                CorrectFormat+=1\n",
    "                QuestionsAnswers[number]=context\n",
    "    Correct.append(CorrectFormat==ExpectedQuestions)\n",
    "    Output.append(QuestionsAnswers)\n",
    "  return (Output,Correct)\n",
    "\n",
    "StartLine=3\n",
    "if \"Qwen2.5\" in ModelFT:\n",
    "    StartLine=2\n",
    "if \"mistral\" in ModelFT:\n",
    "    StartLine=1\n",
    "\n",
    "if Customer==1:\n",
    "  (FTOutput,FTCorrectFormat)=FormatOutput(Solutions,StartLine=StartLine)\n",
    "elif Customer==0:\n",
    "  (FTOutput,FTCorrectFormat)=FormatOutput(Solutions,StartLine=StartLine,ExpectedQuestions=11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a434f65-73bb-4525-b256-ae7546178b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1, 2: 0, 3: 0, 4: 2, 5: 2, 6: 1, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 'Customer inquiring about discrepancy between advertised 35% CDP Data Hub promotion and actual 20% discount rate shown in their account portal, requesting verification of correct promotional rate for 400-node environment deployment.'}\n",
      "500\n",
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parsed formatted output for the finetuned LLM\n",
    "print(FTOutput[12])\n",
    "print(len(FTOutput))\n",
    "print(len(FTCorrectFormat))\n",
    "sum(FTCorrectFormat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91c58ce2-55ad-45d9-9f12-0e938e96935d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. 1',\n",
       " '2. 0',\n",
       " '3. 0',\n",
       " '4. 2',\n",
       " '5. 2',\n",
       " '6. 0',\n",
       " '7. 0',\n",
       " '8. 0',\n",
       " '9. 0',\n",
       " '10. 0',\n",
       " '11. 0',\n",
       " '12. 0',\n",
       " '13. 0',\n",
       " '14. 0',\n",
       " '15. 0',\n",
       " '16. 0',\n",
       " '17. Customer inquiring about using NVMe drives in HP Proliant DL380 Gen10 servers for Cloudera cluster deployment focused on real-time analytics, requesting recommendations for number of NVMe drives per node for optimal performance.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parsed formatted output for the finetuned LLM\n",
    "Solutions[2].split('\\n')[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9c20632-9d91-42fe-9d68-8b289da67b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "StartLine=3\n",
    "if \"Qwen2.5\" in ModelBase:\n",
    "    StartLine=2\n",
    "if \"mistral\" in ModelBase:\n",
    "    StartLine=1\n",
    "\n",
    "if Customer==1:\n",
    "  (BaselineOutput,BaselineCorrectFormat)=FormatOutput(SolutionsBase,StartLine=StartLine)\n",
    "elif Customer==0:\n",
    "  (BaselineOutput,BaselineCorrectFormat)=FormatOutput(SolutionsBase,StartLine=StartLine,ExpectedQuestions=11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43d7b136-f25b-4bc8-a34c-c3090e4bd478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "500\n",
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parsed formatted output for the base LLM\n",
    "\n",
    "print(BaselineOutput[2])\n",
    "print(len(BaselineOutput))\n",
    "print(len(BaselineCorrectFormat))\n",
    "sum(BaselineCorrectFormat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8820c54e-4c32-4531-a1b4-7868c8ba51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompts and 3 examples for LLM-as-a-judge to score whether model A or B wins or if there is a tie.\n",
    "\n",
    "\n",
    "comment=\"Hi Support team, we're experiencing critical performance issues with our Hive queries in our production environment. All queries are taking 3-4 times longer than usual to execute, severely impacting our business operations. Can you please escalate this to a high priority case and have someone look into this ASAP? We need immediate assistance as this is affecting our SLAs.\"\n",
    "question=\"\"\"Does this comment relate to a customer complaint? (BOOL)\"\"\"\n",
    "AnswerA=\"\"\"0\"\"\"\n",
    "AnswerB=\"\"\"1\"\"\"\n",
    "\n",
    "Example1Prompt=\"You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \\n\" + comment +\".\\nHere is the question:\\n\" + question+\":\\n\" +\"\\nHere is the answer of System A:\\n\"+ AnswerA +\"\\nHere is the answer of System B:\\n\"+AnswerB\n",
    "\n",
    "Example1Completion=\"\"\"```json\n",
    "{\n",
    "    \"choice\": \"B\"\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "comment=\"We're experiencing issues with our Cloudera Data Warehouse environment where our Impala queries are failing with OOM errors. The cluster metrics show high memory utilization across all nodes. This is blocking our critical reporting pipeline. Could you please help investigate this issue? We need this resolved within the next 24 hours.\"\n",
    "question=\"\"\"Customer complaint temperature or a frustration level (score 1-4, 0 if not a complaint)\"\"\"\n",
    "AnswerA=\"\"\"3\"\"\"\n",
    "AnswerB=\"\"\"3\"\"\"\n",
    "\n",
    "Example2Prompt=\"You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \\n\" + comment +\".\\nHere is the question:\\n\" + question+\":\\n\" +\"\\nHere is the answer of System A:\\n\"+ AnswerA +\"\\nHere is the answer of System B:\\n\"+AnswerB\n",
    "\n",
    "Example2Completion=\"\"\"```json\n",
    "{\n",
    "    \"choice\": \"tie\"\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "comment=\"Our Cloudera Manager is showing multiple critical alerts for HDFS data nodes. The replication factor has dropped below the minimum threshold, and we're seeing increased latency in our data processing jobs. Can you please help us resolve this urgently? This is impacting our production workloads.\"\n",
    "question=\"\"\"Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]\"\"\"\n",
    "AnswerA=\"\"\"Critical HDFS issues with data nodes showing alerts, reduced replication factor causing increased latency in production data processing jobs.\"\"\"\n",
    "AnswerB=\"\"\"This is about critical alets.\"\"\"\n",
    "\n",
    "Example3Prompt=\"You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \\n\" + comment +\".\\nHere is the question:\\n\" + question+\":\\n\" +\"\\nHere is the answer of System A:\\n\"+ AnswerA +\"\\nHere is the answer of System B:\\n\"+AnswerB\n",
    "Example3Completion=\"\"\"```json\n",
    "{\n",
    "    \"choice\": \"A\"\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09e765f0-b4dc-4016-93ca-36db519bd5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat2 = [\n",
    "  {\"role\": \"system\", \"content\": System},\n",
    "\n",
    "  {\"role\": \"user\", \"content\": Example1Prompt},\n",
    "  {\"role\": \"assistant\", \"content\": Example1Completion},\n",
    "  {\"role\": \"user\", \"content\": Example2Prompt},\n",
    "  {\"role\": \"assistant\", \"content\": Example2Completion},\n",
    "  {\"role\": \"user\", \"content\": Example3Prompt},\n",
    "  {\"role\": \"assistant\", \"content\": Example3Completion}\n",
    "]\n",
    "\n",
    "Completion=\"\"\"```json\n",
    "{\n",
    "    \"choice\": \"\"\"\n",
    "\n",
    "chat=chat2.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6c95da6-8f65-4251-8810-e4c18d3e2adc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "[100264, 9125, 100266, 2675, 527, 264, 11190, 18328, 13, 100265, 100264, 882, 100266, 2675, 527, 2728, 264, 4068, 323, 264, 3488, 922, 279, 4068, 13, 1472, 690, 387, 2728, 279, 2077, 315, 1403, 6067, 323, 499, 1205, 311, 11913, 902, 1887, 220, 6688, 279, 1888, 2077, 13, 7557, 2771, 499, 3493, 279, 4320, 304, 3024, 323, 701, 5873, 374, 2728, 304, 264, 2115, 449, 279, 907, 5873, 13, 5810, 374, 279, 4068, 25, 720, 13347, 9365, 2128, 11, 584, 2351, 25051, 9200, 5178, 4819, 449, 1057, 69278, 20126, 304, 1057, 5788, 4676, 13, 2052, 20126, 527, 4737, 220, 18, 12, 19, 3115, 5129, 1109, 13783, 311, 9203, 11, 35906, 74055, 1057, 2626, 7677, 13, 3053, 499, 4587, 89690, 420, 311, 264, 1579, 10844, 1162, 323, 617, 4423, 1427, 1139, 420, 67590, 30, 1226, 1205, 14247, 13291, 439, 420, 374, 28987, 1057, 17216, 2170, 35047, 8586, 374, 279, 3488, 512, 22186, 420, 4068, 29243, 311, 264, 6130, 12458, 30, 320, 10611, 7887, 8586, 374, 279, 4320, 315, 744, 362, 512, 15, 198, 8586, 374, 279, 4320, 315, 744, 426, 512, 16, 100265, 100264, 78191, 100266, 74694, 2285, 198, 517, 262, 330, 12008, 794, 330, 33, 702, 534, 74694, 100265, 100264, 882, 100266, 2675, 527, 2728, 264, 4068, 323, 264, 3488, 922, 279, 4068, 13, 1472, 690, 387, 2728, 279, 2077, 315, 1403, 6067, 323, 499, 1205, 311, 11913, 902, 1887, 220, 6688, 279, 1888, 2077, 13, 7557, 2771, 499, 3493, 279, 4320, 304, 3024, 323, 701, 5873, 374, 2728, 304, 264, 2115, 449, 279, 907, 5873, 13, 5810, 374, 279, 4068, 25, 720, 1687, 2351, 25051, 4819, 449, 1057, 2493, 283, 99003, 2956, 52466, 4676, 1405, 1057, 14727, 6181, 20126, 527, 22109, 449, 507, 1937, 6103, 13, 578, 10879, 17150, 1501, 1579, 5044, 50549, 4028, 682, 7954, 13, 1115, 374, 22978, 1057, 9200, 13122, 15660, 13, 16910, 499, 4587, 1520, 19874, 420, 4360, 30, 1226, 1205, 420, 20250, 2949, 279, 1828, 220, 1187, 4207, 35047, 8586, 374, 279, 3488, 512, 13084, 12458, 9499, 477, 264, 33086, 2237, 320, 12618, 220, 16, 12, 19, 11, 220, 15, 422, 539, 264, 12458, 7887, 8586, 374, 279, 4320, 315, 744, 362, 512, 18, 198, 8586, 374, 279, 4320, 315, 744, 426, 512, 18, 100265, 100264, 78191, 100266, 74694, 2285, 198, 517, 262, 330, 12008, 794, 330, 49831, 702, 534, 74694, 100265, 100264, 882, 100266, 2675, 527, 2728, 264, 4068, 323, 264, 3488, 922, 279, 4068, 13, 1472, 690, 387, 2728, 279, 2077, 315, 1403, 6067, 323, 499, 1205, 311, 11913, 902, 1887, 220, 6688, 279, 1888, 2077, 13, 7557, 2771, 499, 3493, 279, 4320, 304, 3024, 323, 701, 5873, 374, 2728, 304, 264, 2115, 449, 279, 907, 5873, 13, 5810, 374, 279, 4068, 25, 720, 8140, 2493, 283, 99003, 10790, 374, 9204, 5361, 9200, 30350, 369, 473, 63366, 828, 7954, 13, 578, 48891, 8331, 706, 12504, 3770, 279, 8187, 12447, 11, 323, 584, 2351, 9298, 7319, 40370, 304, 1057, 828, 8863, 7032, 13, 3053, 499, 4587, 1520, 603, 9006, 420, 77720, 30, 1115, 374, 74055, 1057, 5788, 990, 33785, 35047, 8586, 374, 279, 3488, 512, 9370, 5730, 553, 279, 1162, 4068, 9955, 49205, 433, 439, 1790, 439, 3284, 719, 2085, 13490, 3062, 11156, 3649, 13, 507, 1800, 2737, 904, 6574, 22114, 2038, 13, 220, 510, 12998, 69662, 8586, 374, 279, 4320, 315, 744, 362, 512, 43108, 473, 63366, 4819, 449, 828, 7954, 9204, 30350, 11, 11293, 48891, 8331, 14718, 7319, 40370, 304, 5788, 828, 8863, 7032, 627, 8586, 374, 279, 4320, 315, 744, 426, 512, 2028, 374, 922, 9200, 264, 10145, 13, 100265, 100264, 78191, 100266, 74694, 2285, 198, 517, 262, 330, 12008, 794, 330, 32, 702, 534, 74694, 100265, 100264, 882, 100266, 2675, 527, 2728, 264, 4068, 323, 264, 3488, 922, 279, 4068, 13, 1472, 690, 387, 2728, 279, 2077, 315, 1403, 6067, 11, 1887, 362, 323, 1887, 426, 323, 499, 1205, 311, 11913, 902, 1887, 220, 6688, 279, 1888, 2077, 477, 422, 433, 574, 264, 18623, 13, 7557, 2771, 499, 3493, 279, 4320, 304, 3024, 323, 701, 5873, 374, 2728, 304, 264, 2115, 449, 279, 907, 5873, 13, 5810, 374, 279, 4068, 25, 720, 13347, 2493, 283, 99003, 9365, 11, 358, 2846, 25051, 459, 4360, 449, 1057, 356, 10510, 4676, 1405, 279, 3313, 62987, 287, 374, 87657, 1886, 7167, 13, 5751, 11298, 2320, 8948, 7309, 7032, 527, 4737, 1790, 5129, 1109, 13783, 1606, 279, 10879, 4536, 956, 28041, 704, 10489, 13, 1226, 3077, 20336, 264, 28041, 4947, 449, 264, 2218, 14266, 50549, 315, 220, 1399, 13689, 719, 279, 502, 7954, 7784, 956, 1694, 3779, 1524, 994, 1510, 50549, 13280, 220, 1490, 14697, 578, 1515, 6492, 5039, 912, 8196, 6103, 11, 719, 420, 374, 28987, 1057, 8863, 25845, 13, 16910, 499, 4587, 3493, 1063, 19351, 389, 69771, 1521, 3313, 62987, 287, 4819, 30, 627, 8586, 374, 279, 3488, 512, 9370, 5730, 553, 279, 1162, 4068, 9955, 49205, 433, 439, 1790, 439, 3284, 719, 2085, 13490, 3062, 11156, 3649, 13, 507, 1800, 2737, 904, 6574, 22114, 2038, 13, 220, 510, 12998, 933, 8586, 374, 279, 4320, 315, 744, 362, 1473, 8586, 374, 279, 4320, 315, 744, 426, 512, 13084, 6821, 4819, 449, 356, 10510, 3313, 62987, 287, 1405, 11298, 2320, 8948, 7309, 7032, 527, 4737, 5129, 1109, 13783, 4245, 311, 41086, 10879, 28041, 13, 578, 10879, 4536, 956, 28041, 704, 8994, 1579, 14266, 50549, 320, 1490, 34971, 8994, 3515, 264, 28041, 4947, 20336, 369, 220, 1399, 4, 2218, 50549, 13, 100265, 100264, 78191, 100266, 74694, 2285, 198, 517, 262, 330, 12008, 794]\n"
     ]
    }
   ],
   "source": [
    "#Setup LLM-as-a-judge final prompt\n",
    "from transformers import AutoTokenizer\n",
    "EvalLLM='microsoft/phi-4'\n",
    "RemoveAssistant=2\n",
    "if EvalLLM=='microsoft/phi-4':\n",
    "    RemoveAssistant=1\n",
    "elif EvalLLM=='Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8':\n",
    "    RemoveAssistant=2\n",
    "#tokenizer = AutoTokenizer.from_pretrained(EvalLLM)\n",
    "tokenizer = AutoTokenizer.from_pretrained(EvalLLM)\n",
    "\n",
    "if Customer==1:   \n",
    "    ExpectedQuestions=17\n",
    "\n",
    "elif Customer==0:\n",
    "    ExpectedQuestions=11\n",
    "    \n",
    "FTCorrectFormat={}\n",
    "BaselineCorrectFormat={}\n",
    "AllPrompts=[]\n",
    "\n",
    "for i in range(1,ExpectedQuestions+1):\n",
    "    FTCorrectFormat[i]={}\n",
    "    BaselineCorrectFormat[i]={}\n",
    "    Prompts=[]\n",
    "    print(i)\n",
    "    for j in range(0,len(FTOutput)):\n",
    "        chat=chat2.copy()\n",
    "        \n",
    "        if i in FTOutput[j]:\n",
    "            FTCandidate=str(FTOutput[j][i])\n",
    "            FTCorrectFormat[i][j]=True\n",
    "        else:\n",
    "            FTCandidate=\"\"\n",
    "            FTCorrectFormat[i][j]=False\n",
    "            \n",
    "        if i in BaselineOutput[j]:\n",
    "            BaselineCandidate=str(BaselineOutput[j][i])\n",
    "            BaselineCorrectFormat[i][j]=True\n",
    "        else:\n",
    "            BaselineCandidate=\"\"\n",
    "            BaselineCorrectFormat[i][j]=False\n",
    "        user=\"\"\n",
    "        if j%2==0:\n",
    "          user=\"You are given a comment and a question about the comment. You will be given the response of two systems, system A and system B and you need to judge which system  gave the best response or if it was a tie. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \\n\" + Comment[j]+\".\\nHere is the question:\\n\"+FixedQuestions[i]+\"\\nHere is the answer of System A:\\n\" + FTCandidate +\"\\nHere is the answer of System B:\\n\"+BaselineCandidate + \"\\n\"\n",
    "        else:\n",
    "          user=\"You are given a comment and a question about the comment. You will be given the response of two systems, system A and system B and you need to judge which system  gave the best response or if it was a tie. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \\n\" + Comment[j]+\".\\nHere is the question:\\n\"+FixedQuestions[i]+\"\\nHere is the answer of System A:\\n\" + BaselineCandidate +\"\\nHere is the answer of System B:\\n\"+FTCandidate + \"\\n\"\n",
    "\n",
    "        chat.append({\"role\": \"user\", \"content\": user})\n",
    "        chat.append({\"role\": \"assistant\", \"content\": Completion})\n",
    "        \n",
    "        Prompt=tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "        Prompt=tokenizer.decode(tokenizer.encode(Prompt)[:-RemoveAssistant])\n",
    "        Prompts.append(Prompt)\n",
    "    AllPrompts.append(Prompts)\n",
    "    \n",
    "\n",
    "\n",
    "print(tokenizer.encode(Prompts[1]))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ac083d5-82d9-460f-8d64-6341b2303901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system<|im_sep|>You are a helpful assistant.<|im_end|><|im_start|>user<|im_sep|>You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \n",
      "Hi Support team, we're experiencing critical performance issues with our Hive queries in our production environment. All queries are taking 3-4 times longer than usual to execute, severely impacting our business operations. Can you please escalate this to a high priority case and have someone look into this ASAP? We need immediate assistance as this is affecting our SLAs..\n",
      "Here is the question:\n",
      "Does this comment relate to a customer complaint? (BOOL):\n",
      "\n",
      "Here is the answer of System A:\n",
      "0\n",
      "Here is the answer of System B:\n",
      "1<|im_end|><|im_start|>assistant<|im_sep|>```json\n",
      "{\n",
      "    \"choice\": \"B\"\n",
      "}\n",
      "```<|im_end|><|im_start|>user<|im_sep|>You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \n",
      "We're experiencing issues with our Cloudera Data Warehouse environment where our Impala queries are failing with OOM errors. The cluster metrics show high memory utilization across all nodes. This is blocking our critical reporting pipeline. Could you please help investigate this issue? We need this resolved within the next 24 hours..\n",
      "Here is the question:\n",
      "Customer complaint temperature or a frustration level (score 1-4, 0 if not a complaint):\n",
      "\n",
      "Here is the answer of System A:\n",
      "3\n",
      "Here is the answer of System B:\n",
      "3<|im_end|><|im_start|>assistant<|im_sep|>```json\n",
      "{\n",
      "    \"choice\": \"tie\"\n",
      "}\n",
      "```<|im_end|><|im_start|>user<|im_sep|>You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \n",
      "Our Cloudera Manager is showing multiple critical alerts for HDFS data nodes. The replication factor has dropped below the minimum threshold, and we're seeing increased latency in our data processing jobs. Can you please help us resolve this urgently? This is impacting our production workloads..\n",
      "Here is the question:\n",
      "Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]:\n",
      "\n",
      "Here is the answer of System A:\n",
      "Critical HDFS issues with data nodes showing alerts, reduced replication factor causing increased latency in production data processing jobs.\n",
      "Here is the answer of System B:\n",
      "This is about critical alets.<|im_end|><|im_start|>assistant<|im_sep|>```json\n",
      "{\n",
      "    \"choice\": \"A\"\n",
      "}\n",
      "```<|im_end|><|im_start|>user<|im_sep|>You are given a comment and a question about the comment. You will be given the response of two systems, system A and system B and you need to judge which system  gave the best response or if it was a tie. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \n",
      "Hi Cloudera Support Team,\n",
      "\n",
      "    I need assistance with our CDE Virtual Cluster's network configuration. We're trying to connect our Spark jobs to an external data source through a custom VPC endpoint, but the jobs keep failing with connection timeout errors. We've verified the security group settings and network ACLs are correct. This is blocking our development team from accessing critical third-party data feeds.\n",
      "\n",
      "    Could you please help us resolve this configuration issue?\n",
      "\n",
      "    Thanks,\n",
      "    Kevin.\n",
      "Here is the question:\n",
      "Does this comment discuss any technical information? (BOOL: 0 for no, 1 for yes)\n",
      "Here is the answer of System A:\n",
      "\n",
      "Here is the answer of System B:\n",
      "1<|im_end|><|im_start|>assistant<|im_sep|>```json\n",
      "{\n",
      "    \"choice\":\n"
     ]
    }
   ],
   "source": [
    "#LLM-as-a-judge prompt example\n",
    "print(AllPrompts[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c621694-d7ae-4825-a63a-04fc9abed761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-22 08:42:17 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 03-22 08:42:17 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='microsoft/phi-4', speculative_config=None, tokenizer='microsoft/phi-4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/phi-4, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 03-22 08:42:18 model_runner.py:1060] Starting to load model microsoft/phi-4...\n",
      "INFO 03-22 08:42:19 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:00<00:03,  1.40it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:01<00:02,  1.34it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:02<00:02,  1.36it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:02<00:01,  1.38it/s]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:03<00:00,  1.39it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:04<00:00,  1.42it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:04<00:00,  1.40it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-22 08:42:25 model_runner.py:1071] Loading model weights took 27.3875 GB\n",
      "INFO 03-22 08:42:27 gpu_executor.py:122] # GPU blocks: 702, # CPU blocks: 1310\n",
      "INFO 03-22 08:42:27 gpu_executor.py:126] Maximum concurrency for 10000 tokens per request: 1.12x\n"
     ]
    }
   ],
   "source": [
    "#Load LLM-as-a-judge\n",
    "max_num_seqs=1\n",
    "max_model_len=10000\n",
    "max_num_batched_tokens=10000 \n",
    "gpu_memory_utilization=0.7\n",
    "enforce_eager=True\n",
    "tensor_parallel_size=1\n",
    "seed=None\n",
    "max_tokens=10\n",
    "temperature=0.1\n",
    "\n",
    "import vllm\n",
    "llm = vllm.LLM(model=EvalLLM, max_num_seqs=max_num_seqs,max_model_len=max_model_len,max_num_batched_tokens=max_num_batched_tokens, gpu_memory_utilization=gpu_memory_utilization,enforce_eager=enforce_eager, tensor_parallel_size=tensor_parallel_size)\n",
    "seed=None\n",
    "max_tokens=max_tokens\n",
    "temperature=temperature\n",
    "sampling_params = vllm.SamplingParams(seed=None,max_tokens=max_tokens,temperature=temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffb2b0ac-8329-4db3-94a3-eb72e5c76a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:48<00:00,  2.96it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 1\n",
      "===================\n",
      "99.4%\n",
      "0.6%\n",
      "0.994\n",
      "0.006\n",
      "497\n",
      "3\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:48<00:00,  2.96it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 2\n",
      "===================\n",
      "99.4%\n",
      "0.6%\n",
      "0.994\n",
      "0.006\n",
      "497\n",
      "3\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:59<00:00,  2.79it/s, est. speed input: 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 3\n",
      "===================\n",
      "88.75502008032129%\n",
      "11.244979919678714%\n",
      "0.8875502008032129\n",
      "0.11244979919678715\n",
      "442\n",
      "56\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [03:16<00:00,  2.54it/s, est. speed input: 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 4\n",
      "===================\n",
      "96.5863453815261%\n",
      "3.413654618473896%\n",
      "0.9658634538152611\n",
      "0.03413654618473896\n",
      "481\n",
      "17\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [03:17<00:00,  2.53it/s, est. speed input: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 5\n",
      "===================\n",
      "91.56626506024097%\n",
      "8.433734939759036%\n",
      "0.912\n",
      "0.084\n",
      "456\n",
      "42\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [03:24<00:00,  2.44it/s, est. speed input: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 6\n",
      "===================\n",
      "55.2%\n",
      "44.800000000000004%\n",
      "0.552\n",
      "0.448\n",
      "276\n",
      "224\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:49<00:00,  2.96it/s, est. speed input: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 7\n",
      "===================\n",
      "99.6%\n",
      "0.4%\n",
      "0.996\n",
      "0.004\n",
      "498\n",
      "2\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [03:26<00:00,  2.42it/s, est. speed input: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 8\n",
      "===================\n",
      "54.2%\n",
      "45.800000000000004%\n",
      "0.542\n",
      "0.458\n",
      "271\n",
      "229\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:58<00:00,  2.81it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 9\n",
      "===================\n",
      "87.1584699453552%\n",
      "12.841530054644808%\n",
      "0.638\n",
      "0.094\n",
      "319\n",
      "47\n",
      "134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [03:02<00:00,  2.74it/s, est. speed input: 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 10\n",
      "===================\n",
      "82.32931726907631%\n",
      "17.670682730923694%\n",
      "0.82\n",
      "0.176\n",
      "410\n",
      "88\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [03:05<00:00,  2.70it/s, est. speed input: 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 11\n",
      "===================\n",
      "78.4%\n",
      "21.6%\n",
      "0.784\n",
      "0.216\n",
      "392\n",
      "108\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [03:28<00:00,  2.40it/s, est. speed input: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 12\n",
      "===================\n",
      "50.4%\n",
      "49.6%\n",
      "0.504\n",
      "0.496\n",
      "252\n",
      "248\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:57<00:00,  2.82it/s, est. speed input: 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 13\n",
      "===================\n",
      "92.2%\n",
      "7.8%\n",
      "0.922\n",
      "0.078\n",
      "461\n",
      "39\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:48<00:00,  2.96it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 14\n",
      "===================\n",
      "99.79959919839679%\n",
      "0.2004008016032064%\n",
      "0.996\n",
      "0.002\n",
      "498\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [03:22<00:00,  2.47it/s, est. speed input: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 15\n",
      "===================\n",
      "58.599999999999994%\n",
      "41.4%\n",
      "0.586\n",
      "0.414\n",
      "293\n",
      "207\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [03:11<00:00,  2.62it/s, est. speed input: 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 16\n",
      "===================\n",
      "68.80530973451327%\n",
      "31.194690265486724%\n",
      "0.622\n",
      "0.282\n",
      "311\n",
      "141\n",
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:51<00:00,  2.92it/s, est. speed input: 25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 17\n",
      "===================\n",
      "99.79919678714859%\n",
      "0.2008032128514056%\n",
      "0.9979919678714859\n",
      "0.002008032128514056\n",
      "497\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Run LLM-as-a-judge for eacb question and at the same time count correct answers/ties.\n",
    "Outs=[]\n",
    "PosPercNoTies=[]\n",
    "NegPercNoTies=[]\n",
    "PosPercTies=[]\n",
    "NegPercTies=[]\n",
    "countPosArr=[]\n",
    "countNegArr=[]\n",
    "countTieArr=[]\n",
    "TiesPerc=[]\n",
    "\n",
    "for j in range(0,ExpectedQuestions):\n",
    "  max_num_seqs=16\n",
    "\n",
    "  sampling_params = vllm.SamplingParams(seed=1,max_tokens=max_tokens,temperature=temperature)\n",
    "\n",
    "  Out=llm.generate(AllPrompts[j], sampling_params)\n",
    "  Outs.append(Out)\n",
    "  countPos=0\n",
    "  countNeg=0\n",
    "  countTie=0\n",
    "\n",
    "  for i in range(0,len(Out)):\n",
    "    if i%2==0:\n",
    "      #print(i)\n",
    "      #print(Out[i].prompt)\n",
    "      #print(Out[i].outputs[0].text[2])\n",
    "      if len(Out[i].outputs[0].text)<2:\n",
    "        continue\n",
    "\n",
    "      if Out[i].outputs[0].text[2]=='A':\n",
    "        countPos+=1\n",
    "      elif Out[i].outputs[0].text[2]=='B':\n",
    "        countNeg+=1\n",
    "      elif Out[i].outputs[0].text[2]=='t':\n",
    "        countTie+=1\n",
    "\n",
    "    if i%2==1:\n",
    "      if len(Out[i].outputs[0].text)<2:\n",
    "        continue\n",
    "\n",
    "      if Out[i].outputs[0].text[2]=='B':\n",
    "        countPos+=1\n",
    "      elif Out[i].outputs[0].text[2]=='A':\n",
    "        countNeg+=1\n",
    "      elif Out[i].outputs[0].text[2]=='t':\n",
    "        countTie+=1\n",
    "  print(\"===================\")\n",
    "  print(\"Question \"+str(j+1))\n",
    "  print(\"===================\")\n",
    "  print(str(countPos/(countPos+countNeg)*100)+\"%\")\n",
    "  print(str(countNeg/(countPos+countNeg)*100)+\"%\")\n",
    "  print(countPos/(countPos+countNeg+countTie))\n",
    "  print(countNeg/(countPos+countNeg+countTie))\n",
    "\n",
    "  print(countPos)\n",
    "  print(countNeg)\n",
    "  print(countTie)\n",
    "  PosPercNoTies.append(str(countPos/(countPos+countNeg)*100)+\"%\")\n",
    "  NegPercNoTies.append(str(countNeg/(countPos+countNeg)*100)+\"%\")\n",
    "  TiesPerc.append(countTie/(countPos+countNeg+countTie))\n",
    "  PosPercTies.append(countPos/(countPos+countNeg+countTie))\n",
    "  NegPercTies.append(countNeg/(countPos+countNeg+countTie))\n",
    "  countPosArr.append(countPos)\n",
    "  countNegArr.append(countNeg)\n",
    "  countTieArr.append(countTie)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6efe9958-6af1-4e88-a15e-36751c90d739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.4%\n",
      "99.4%\n",
      "88.75502008032129%\n",
      "96.5863453815261%\n",
      "91.56626506024097%\n",
      "55.2%\n",
      "99.6%\n",
      "54.2%\n",
      "87.1584699453552%\n",
      "82.32931726907631%\n",
      "78.4%\n",
      "50.4%\n",
      "92.2%\n",
      "99.79959919839679%\n",
      "58.599999999999994%\n",
      "68.80530973451327%\n",
      "99.79919678714859%\n"
     ]
    }
   ],
   "source": [
    "#Compute winrate\n",
    "for i in PosPercNoTies:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93135d09-9cc9-48a0-942a-2a0136e93d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6%\n",
      "0.6%\n",
      "11.244979919678714%\n",
      "3.413654618473896%\n",
      "8.433734939759036%\n",
      "44.800000000000004%\n",
      "0.4%\n",
      "45.800000000000004%\n",
      "12.841530054644808%\n",
      "17.670682730923694%\n",
      "21.6%\n",
      "49.6%\n",
      "7.8%\n",
      "0.2004008016032064%\n",
      "41.4%\n",
      "31.194690265486724%\n",
      "0.2008032128514056%\n"
     ]
    }
   ],
   "source": [
    "#Compute lose rate!\n",
    "for i in NegPercNoTies:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54cfbd2a-6931-4f78-8e62-45201e5b2174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n",
      "0.0%\n",
      "0.0%\n",
      "0.0%\n",
      "0.4%\n",
      "0.0%\n",
      "0.0%\n",
      "0.0%\n",
      "26.8%\n",
      "0.4%\n",
      "0.0%\n",
      "0.0%\n",
      "0.0%\n",
      "0.2%\n",
      "0.0%\n",
      "9.6%\n",
      "0.0%\n"
     ]
    }
   ],
   "source": [
    "#Compute ties.\n",
    "for i in TiesPerc:\n",
    "    print(str(i*100) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
