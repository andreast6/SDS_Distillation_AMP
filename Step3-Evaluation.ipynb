{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1a25959-2431-4178-8865-c26c7fcb2a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup inference and evaluation parameters\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "GPU_ID=\"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=GPU_ID\n",
    "\n",
    "Customer=1\n",
    "CustomPrompt=1\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "import Libs.DataSynthesisLib as DataSynthesisLib\n",
    "Config=DataSynthesisLib.GetConfig('Config/Config.py')\n",
    "\n",
    "ModelFT='/tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05'\n",
    "ModelBase='unsloth/Meta-Llama-3.1-8B-Instruct'\n",
    "\n",
    "\n",
    "AssistantPromptElementsRemoval=2\n",
    "EvalLLM='microsoft/phi-4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851eca19-e15c-4621-a9b5-03fd195bd607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load appropriate evaluation dataset\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "if Customer==1:\n",
    "  with open('Data/CustomerComments_Evaluation_Clean.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "elif Customer==0:\n",
    "  with open('Data/ClouderaComments_Evaluation_Clean.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d6ca99-405d-4a7b-8d49-62f92918d507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#Setup questions for LLM-as-a-judge, collect prompts for inference, and customer or Cloudera comments to be used later for evaluation\n",
    "import vllm\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(ModelFT)\n",
    "\n",
    "System='You are a helpful assistant.'\n",
    "\n",
    "\n",
    "FixedQuestions={}\n",
    "if Customer==1:\n",
    "  FixedQuestions[1]=\"Does this comment discuss any technical information? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[2]=\"Does this comment relate to a customer complaint? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[3]=\"Customer complaint temperature or a frustration level (if there is a complain give a Score 1-4, lowest=1, highest=4. If there is no complain give a score of 0)\"\n",
    "  FixedQuestions[4]=\"Score the severity of the issue based on comment content (SCORE 1-4, lowest=1, highest=4)\"\n",
    "  FixedQuestions[5]=\"Score the urgency of the issue based on the comment content (SCORE 1-4, lowest=1, highest=4)\"\n",
    "  FixedQuestions[6]=\"Is this a request from a customer for an update? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[7]=\"Is there a strictly explicit and NOT an implied request from a customer for a call, meeting or a screenshare (zoom/webex/teams etc.)? Do not answer yes unless wording explicitly asks for a call. (BOOL)\"\n",
    "  FixedQuestions[8]=\"Did the customer request an escalation? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[9]=\"Did the customer request a priority change?  To what level? (If there is a priority change give score 1 to indicate highest priority (indicated by S1) and 4  to indicate the lowest priority (Indicated by 4). If there is no priority change give a score of 0).\"\n",
    "  FixedQuestions[10]=\"Did the customer request a transfer to another COE? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[11]=\"Did the customer request to speak to a manager or supervisor? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[12]=\"Did the customer request a SME or expert? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[13]=\"Does this comment discuss a bug in Cloudera software? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[14]=\"Does the comment include a non-Cloudera Apache JIRA link? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[15]=\"Does the comment have a link to Cloudera Documentation or Community article? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[16]=\"Does the comment have any other type of hyperlink? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[17]=\"Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]\"\n",
    "elif Customer==0:\n",
    "  FixedQuestions[1]=\"Does this comment discuss any technical information? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[2]=\"Score the severity of the issue based on comment content (SCORE 1-4, lowest=1, highest=4)\"\n",
    "  FixedQuestions[3]=\"Score the urgency of the issue based on the comment content (SCORE 1-4, lowest=1, highest=4)\"\n",
    "  FixedQuestions[4]=\"Does the comment have a proposed solution? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[5]=\"Does the comment have a proposed workaround?  (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[6]=\"Does the comment have a request for an action from the customer?  (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[7]=\"Does this comment discuss a bug in Cloudera software? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[8]=\"Does the comment include a non-Cloudera Apache JIRA link? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[9]=\"Does the comment have a link to Cloudera Documentation or Community article? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[10]=\"Does the comment have any other type of hyperlink? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[11]=\"Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT] \"\n",
    "\n",
    "\n",
    "Questions=[]\n",
    "Comment=[]\n",
    "BaselineSolutions=[]\n",
    "for i in data:\n",
    "  if Customer==1:\n",
    "      \n",
    "    if CustomPrompt==1:\n",
    "\n",
    "      Questions.append(i['Prompt'])\n",
    "      Comment.append(i['Comment'])\n",
    "\n",
    "  elif Customer==0:\n",
    "\n",
    "    CommentText=\"\"\"You are given a Cloudera support team comment, 11 questions referring to the comment, and the possible values for each question in parenthesis following the question. Here is the Cloudera comment:\n",
    "\n",
    "\"\"\"\n",
    "    PreppendClouderaQuestions=\"\"\"\n",
    "\n",
    "Here are the 11 questions:\n",
    "1. Does this comment discuss any technical information? (answer 0 for no, 1 for yes)\n",
    "2. Score the severity of the issue based on comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
    "3. Score the urgency of the issue based on the comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
    "4. Does the comment have a proposed solution? (answer 0 for no, 1 for yes)\n",
    "5. Does the comment have a proposed workaround?  (answer 0 for no, 1 for yes)\n",
    "6. Does the comment have a request for an action from the customer?  (answer 0 for no, 1 for yes)\n",
    "7. Does this comment discuss a bug in Cloudera software? (answer 0 for no, 1 for yes)\n",
    "8. Does the comment include a non-Cloudera Apache JIRA link (e.g. a Apache JIRA link with issues.apach.org domain name)? (answer 0 for no, 1 for yes)\n",
    "9. Does the comment have a link to Cloudera Documentation or Community article? (answer 0 for no, 1 for yes)\n",
    "10. Does the comment have any other type of hyperlink? (answer 0 for no, 1 for yes)\n",
    "11. Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]    \n",
    "Generate each answer in a new line using the the format \"Number. answer\" (e.g. 1. 0)\n",
    "\"\"\"\n",
    "    if CustomPrompt==1:\n",
    "      Questions.append(i['Prompt'])\n",
    "      Comment.append(i['Comment'])\n",
    "\n",
    "\n",
    "print(len(Comment))\n",
    "print(len(Questions))\n",
    "lens=[len(i) for i in Questions ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c9bc38-2e58-4e5e-9cc4-c1ca44dfe7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a customer comment, 17 questions referring to the comment, and the possible values for each question in parenthesis following the question. Here is the customer comment:\n",
      "\n",
      "Hi Cloudera Support Team,\n",
      "\n",
      "I noticed that our Ranger policies are not synchronizing properly across our cluster nodes. Several users are reporting access denied errors for resources they should have permissions to access. The Ranger Admin UI shows the policies as properly configured, but they're not being enforced correctly on the services. We've already tried restarting the Ranger admin service and clearing the policy cache, but the issue persists.\n",
      "\n",
      "This is impacting our team's ability to access critical data resources.\n",
      "\n",
      "Best regards,\n",
      "Mark\n",
      "\n",
      "Here are the 17 questions:\n",
      "1. Does this comment discuss any technical information? (answer 0 for no, 1 for yes)\n",
      "2. Does this comment relate to a customer complaint? (answer 0 for no, 1 for yes)\n",
      "3. Customer complaint temperature or a frustration level (if there is a complain give 1 for lowest, 4 for highest and 2,3 for in between. If there is no complain give a score of 0).\n",
      "4. Score the severity of the issue based on comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "5. Score the urgency of the issue based on the comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "6. Is this a request from a customer for an update? (answer 0 for no, 1 for yes)\n",
      "7. Is there a strictly explicit and NOT an implied request from a customer for a call, meeting or a screenshare (zoom/webex/teams etc.)? Do not answer yes unless wording explicitly asks for a call. ((BOOL:0/1)\n",
      "8. Did the customer request an escalation? (answer 0 for no, 1 for yes)\n",
      "9. Did the customer request a priority change?  To what level? (If there is a priority change give score 1 to indicate highest priority (indicated by S1) and 4  to indicate the lowest priority (Indicated by S4). If there is no priority change give a score of 0).\n",
      "10. Did the customer request a transfer to another Customer Operations Engineer? (answer 0 for no, 1 for yes)\n",
      "11. Did the customer request to speak to a manager or supervisor? (answer 0 for no, 1 for yes)\n",
      "12. Did the customer request a Subject Matter Expert or expert? (answer 0 for no, 1 for yes)\n",
      "13. Does this comment discuss a bug in Cloudera software? (answer 0 for no, 1 for yes)\n",
      "14. Does the comment include a non-Cloudera Apache JIRA link (e.g. a Apache JIRA link with issues.apach.org domain name)? (answer 0 for no, 1 for yes)\n",
      "15. Does the comment have a link to Cloudera Documentation or Community article? (answer 0 for no, 1 for yes)\n",
      "16. Does the comment have any other type of hyperlink? (answer 0 for no, 1 for yes)\n",
      "17. Summarize the case comment condensing itas much as possible but without losing important technical details. Omit including any meeting invite information. (TEXT)\n",
      "Generate each answer in a new line using the the format \"Number. answer\" (e.g. 1. 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Typical example on how the LLM is prompted\n",
    "print(Questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c1251c9-4b6c-4112-944c-6cd485af443b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsloth/Meta-Llama-3.1-8B-Instruct\n",
      "WARNING 03-21 19:18:40 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 03-21 19:18:40 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='unsloth/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='unsloth/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 03-21 19:18:42 model_runner.py:1060] Starting to load model unsloth/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 03-21 19:18:42 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.36it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:00,  2.05it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.72it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.48it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.56it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-21 19:18:45 model_runner.py:1071] Loading model weights took 14.9888 GB\n",
      "INFO 03-21 19:18:48 gpu_executor.py:122] # GPU blocks: 6230, # CPU blocks: 0\n",
      "INFO 03-21 19:18:48 gpu_executor.py:126] Maximum concurrency for 32000 tokens per request: 3.12x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 500/500 [02:20<00:00,  3.55it/s, est. speed input: 2688.18 toks/s, output: 466.66 toks/s]\n"
     ]
    }
   ],
   "source": [
    "#Base model parameters setup and inference \n",
    "from torch.multiprocessing import Pool, Process, set_start_method\n",
    "try:\n",
    "     set_start_method('spawn')\n",
    "except RuntimeError:\n",
    "    pass\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "ConfigS=Config[\"ConfigSolution\"].copy()\n",
    "ConfigS[\"gpu-id\"]=GPU_ID\n",
    "ConfigS[\"tensor_parallel_size\"]=\"2\"\n",
    "ConfigS[\"tensor_parallel_size\"]=\"1\"\n",
    "\n",
    "ConfigS[\"system\"]=System\n",
    "ConfigS[\"use-grammar\"]=\"False\"\n",
    "ConfigS[\"AssistantPrompt\"]=\"\"\n",
    "\n",
    "ConfigS[\"max_tokens\"]=\"3000\"\n",
    "ConfigS[\"temperature\"]=\"0.1\"\n",
    "ConfigS[\"max_num_seqs\"]=\"2\"\n",
    "\n",
    "ConfigS[\"max_num_seqs\"]=\"16\"\n",
    "ConfigS[\"max_model_len\"]=\"32000\"\n",
    "ConfigS[\"max_num_batched_tokens\"]=\"32000\"\n",
    "\n",
    "\n",
    "ConfigS[\"AssistantPromptElementsRemoval\"]=\"1\"\n",
    "ConfigS[\"llm-path\"]=ModelBase\n",
    "ConfigS[\"tokenizer-path\"]=ModelBase\n",
    "\n",
    "ConfigS[\"tokenizer-file-gguf\"]=\"\"\n",
    "ConfigS[\"temperature\"]=\"0\"\n",
    "if \"mistral\" in ModelBase:\n",
    "  ConfigS[\"GenerateChatTemplate\"]=\"GenerateChatWithoutSystemWithAssistant\"\n",
    "else:\n",
    "  ConfigS[\"GenerateChatTemplate\"]=\"GenerateChatWithSystem\"\n",
    "\n",
    "ConfigS[\"UseOutlines\"]=\"False\"\n",
    "ConfigS[\"AssistantPromptElementsRemoval\"]=\"1\"\n",
    "ConfigS[\"tokenizer-file-gguf\"]=\"\"\n",
    "if 'gptq' in ModelFT:\n",
    "  ConfigS[\"quantization\"]=\"gptq\"\n",
    "ConfigS[\"gpu_memory_utilization\"]=\"0.7\"\n",
    "\n",
    "\n",
    "(SolutionsBase, SolutionsBaseLogging)=DataSynthesisLib.GetSolutions(ConfigS, Questions, ConfigS[\"system\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f97e1c-98b6-461a-b157-48888cf06427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05\n",
      "WARNING 03-21 19:21:38 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 03-21 19:21:38 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05', speculative_config=None, tokenizer='/tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 03-21 19:21:40 model_runner.py:1060] Starting to load model /tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  6.37it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.27it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.61it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.44it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.64it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-21 19:21:43 model_runner.py:1071] Loading model weights took 14.9888 GB\n",
      "INFO 03-21 19:21:46 gpu_executor.py:122] # GPU blocks: 6230, # CPU blocks: 0\n",
      "INFO 03-21 19:21:46 gpu_executor.py:126] Maximum concurrency for 32000 tokens per request: 3.12x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 500/500 [02:16<00:00,  3.66it/s, est. speed input: 2771.73 toks/s, output: 450.90 toks/s]\n"
     ]
    }
   ],
   "source": [
    "#Finetuned model parameters setup and inference \n",
    "\n",
    "from torch.multiprocessing import Pool, Process, set_start_method\n",
    "try:\n",
    "     set_start_method('spawn')\n",
    "except RuntimeError:\n",
    "    pass\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "ConfigS=Config[\"ConfigSolution\"].copy()\n",
    "ConfigS[\"gpu-id\"]=GPU_ID\n",
    "ConfigS[\"tensor_parallel_size\"]=\"2\"\n",
    "ConfigS[\"tensor_parallel_size\"]=\"1\"\n",
    "\n",
    "ConfigS[\"system\"]=System\n",
    "ConfigS[\"use-grammar\"]=\"False\"\n",
    "ConfigS[\"AssistantPrompt\"]=\"\"\n",
    "\n",
    "ConfigS[\"max_tokens\"]=\"3000\"\n",
    "ConfigS[\"temperature\"]=\"0.1\"\n",
    "ConfigS[\"max_num_seqs\"]=\"2\"\n",
    "\n",
    "ConfigS[\"max_num_seqs\"]=\"16\"\n",
    "ConfigS[\"max_model_len\"]=\"32000\"\n",
    "ConfigS[\"max_num_batched_tokens\"]=\"32000\"\n",
    "\n",
    "\n",
    "ConfigS[\"AssistantPromptElementsRemoval\"]=\"1\"\n",
    "ConfigS[\"llm-path\"]=ModelFT\n",
    "ConfigS[\"tokenizer-path\"]=ModelFT\n",
    "\n",
    "ConfigS[\"tokenizer-file-gguf\"]=\"\"\n",
    "ConfigS[\"temperature\"]=\"0\"\n",
    "if \"mistral\" in ModelFT:\n",
    "\n",
    "  ConfigS[\"GenerateChatTemplate\"]=\"GenerateChatWithoutSystemWithAssistant\"\n",
    "else:\n",
    "  ConfigS[\"GenerateChatTemplate\"]=\"GenerateChatWithSystem\"\n",
    "\n",
    "ConfigS[\"UseOutlines\"]=\"False\"\n",
    "ConfigS[\"AssistantPromptElementsRemoval\"]=\"1\"\n",
    "ConfigS[\"tokenizer-file-gguf\"]=\"\"\n",
    "if 'gptq' in ModelFT:\n",
    "  ConfigS[\"quantization\"]=\"gptq\"\n",
    "ConfigS[\"gpu_memory_utilization\"]=\"0.7\"\n",
    "\n",
    "\n",
    "(Solutions, SolutionsLogging)=DataSynthesisLib.GetSolutions(ConfigS, Questions, ConfigS[\"system\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ecbbdae-d9c1-4b0b-808e-78c67e620a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a customer comment, 17 questions referring to the comment, and the possible values for each question in parenthesis following the question. Here is the customer comment:\n",
      "\n",
      "Hi Cloudera Support Team,\n",
      "\n",
      "I'm experiencing a critical issue with our CDE Virtual Cluster where our nightly data processing jobs are failing with serialization errors. The jobs process around 500GB of data and involve complex transformations. The error occurs during the shuffle phase with 'java.io.NotSerializableException'. We've tried increasing the serializer buffer size but the issue persists. This is impacting our daily reporting requirements.\n",
      "\n",
      "Best regards,\n",
      "Karen\n",
      "\n",
      "Here are the 17 questions:\n",
      "1. Does this comment discuss any technical information? (answer 0 for no, 1 for yes)\n",
      "2. Does this comment relate to a customer complaint? (answer 0 for no, 1 for yes)\n",
      "3. Customer complaint temperature or a frustration level (if there is a complain give 1 for lowest, 4 for highest and 2,3 for in between. If there is no complain give a score of 0).\n",
      "4. Score the severity of the issue based on comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "5. Score the urgency of the issue based on the comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "6. Is this a request from a customer for an update? (answer 0 for no, 1 for yes)\n",
      "7. Is there a strictly explicit and NOT an implied request from a customer for a call, meeting or a screenshare (zoom/webex/teams etc.)? Do not answer yes unless wording explicitly asks for a call. ((BOOL:0/1)\n",
      "8. Did the customer request an escalation? (answer 0 for no, 1 for yes)\n",
      "9. Did the customer request a priority change?  To what level? (If there is a priority change give score 1 to indicate highest priority (indicated by S1) and 4  to indicate the lowest priority (Indicated by S4). If there is no priority change give a score of 0).\n",
      "10. Did the customer request a transfer to another Customer Operations Engineer? (answer 0 for no, 1 for yes)\n",
      "11. Did the customer request to speak to a manager or supervisor? (answer 0 for no, 1 for yes)\n",
      "12. Did the customer request a Subject Matter Expert or expert? (answer 0 for no, 1 for yes)\n",
      "13. Does this comment discuss a bug in Cloudera software? (answer 0 for no, 1 for yes)\n",
      "14. Does the comment include a non-Cloudera Apache JIRA link (e.g. a Apache JIRA link with issues.apach.org domain name)? (answer 0 for no, 1 for yes)\n",
      "15. Does the comment have a link to Cloudera Documentation or Community article? (answer 0 for no, 1 for yes)\n",
      "16. Does the comment have any other type of hyperlink? (answer 0 for no, 1 for yes)\n",
      "17. Summarize the case comment condensing itas much as possible but without losing important technical details. Omit including any meeting invite information. (TEXT)\n",
      "Generate each answer in a new line using the the format \"Number. answer\" (e.g. 1. 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Typical example on how the LLM is prompted\n",
    "print(Questions[43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78a33c1-f752-4eae-b75a-5f5a328895bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1. 1\n",
      "2. 1\n",
      "3. 3\n",
      "4. 4\n",
      "5. 4\n",
      "6. 0\n",
      "7. 0\n",
      "8. 0\n",
      "9. 0\n",
      "10. 0\n",
      "11. 0\n",
      "12. 0\n",
      "13. 1\n",
      "14. 0\n",
      "15. 0\n",
      "16. 0\n",
      "17. Customer reports critical issues with CDE Virtual Cluster where nightly data processing jobs (500GB) fail with serialization errors during shuffle phase, despite increasing serializer buffer size.\n"
     ]
    }
   ],
   "source": [
    "#Example finetuned model output\n",
    "print(Solutions[43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3d33f85-fa4e-4fdc-908a-82978556328d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Does this comment discuss any technical information? (BOOL: 0 for no, 1 for yes)',\n",
       " 2: 'Does this comment relate to a customer complaint? (BOOL: 0 for no, 1 for yes)',\n",
       " 3: 'Customer complaint temperature or a frustration level (if there is a complain give a Score 1-4, lowest=1, highest=4. If there is no complain give a score of 0)',\n",
       " 4: 'Score the severity of the issue based on comment content (SCORE 1-4, lowest=1, highest=4)',\n",
       " 5: 'Score the urgency of the issue based on the comment content (SCORE 1-4, lowest=1, highest=4)',\n",
       " 6: 'Is this a request from a customer for an update? (BOOL: 0 for no, 1 for yes)',\n",
       " 7: 'Is there a strictly explicit and NOT an implied request from a customer for a call, meeting or a screenshare (zoom/webex/teams etc.)? Do not answer yes unless wording explicitly asks for a call. (BOOL)',\n",
       " 8: 'Did the customer request an escalation? (BOOL: 0 for no, 1 for yes)',\n",
       " 9: 'Did the customer request a priority change?  To what level? (If there is a priority change give score 1 to indicate highest priority (indicated by S1) and 4  to indicate the lowest priority (Indicated by 4). If there is no priority change give a score of 0).',\n",
       " 10: 'Did the customer request a transfer to another COE? (BOOL: 0 for no, 1 for yes)',\n",
       " 11: 'Did the customer request to speak to a manager or supervisor? (BOOL: 0 for no, 1 for yes)',\n",
       " 12: 'Did the customer request a SME or expert? (BOOL: 0 for no, 1 for yes)',\n",
       " 13: 'Does this comment discuss a bug in Cloudera software? (BOOL: 0 for no, 1 for yes)',\n",
       " 14: 'Does the comment include a non-Cloudera Apache JIRA link? (BOOL: 0 for no, 1 for yes)',\n",
       " 15: 'Does the comment have a link to Cloudera Documentation or Community article? (BOOL: 0 for no, 1 for yes)',\n",
       " 16: 'Does the comment have any other type of hyperlink? (BOOL: 0 for no, 1 for yes)',\n",
       " 17: 'Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Individual questions for evaluating each question using LLM-as-a-judge\n",
    "FixedQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccb086ce-ca2f-42d3-97d4-8fa3b526c3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1, 2: 1, 3: 3, 4: 4, 5: 4, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 1, 14: 0, 15: 0, 16: 0, 17: 'Customer reports critical performance issues in CDP Enterprise after patch installation, with Hbase operations timing out or failing, showing increased GC pauses and high heap usage, impacting multiple critical applications.'}\n",
      "500\n",
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "485"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parse LLM output and extract analytics and summary\n",
    "def FormatOutput(Solutions,StartLine=3,ExpectedQuestions=17):\n",
    "  CorrectFormat=0\n",
    "  Output=[]\n",
    "  Correct=[]\n",
    "\n",
    "  for d in Solutions:\n",
    "    result=d.split('\\n')\n",
    "    QuestionsAnswers={}\n",
    "    count=0\n",
    "    CorrectFormat=0\n",
    "      \n",
    "    for j in result[StartLine-1:]:\n",
    "      FoundNumber=0\n",
    "      count+=1\n",
    "      context=\"\"\n",
    "      number=j.split('.')[0]\n",
    "      if number.isdigit():\n",
    "          number=int(number)\n",
    "          if number==count:\n",
    "              FoundNumber=1\n",
    "      tmp=j.split(' ')\n",
    "      if len(tmp)>=2:\n",
    "         if count<ExpectedQuestions:\n",
    "             if tmp[1].isdigit():\n",
    "                 context=int(tmp[1])\n",
    "                 if FoundNumber==1:\n",
    "                    CorrectFormat+=1\n",
    "                    QuestionsAnswers[number]=context\n",
    "         else:\n",
    "             context=' '.join(tmp[1:])\n",
    "             if FoundNumber==1:\n",
    "                CorrectFormat+=1\n",
    "                QuestionsAnswers[number]=context\n",
    "    Correct.append(CorrectFormat==ExpectedQuestions)\n",
    "    Output.append(QuestionsAnswers)\n",
    "  return (Output,Correct)\n",
    "\n",
    "StartLine=3\n",
    "if \"Qwen2.5\" in ModelFT:\n",
    "    StartLine=2\n",
    "if \"mistral\" in ModelFT:\n",
    "    StartLine=1\n",
    "\n",
    "if Customer==1:\n",
    "  (FTOutput,FTCorrectFormat)=FormatOutput(Solutions,StartLine=StartLine)\n",
    "elif Customer==0:\n",
    "  (FTOutput,FTCorrectFormat)=FormatOutput(Solutions,StartLine=StartLine,ExpectedQuestions=11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a434f65-73bb-4525-b256-ae7546178b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsed formatted output for the finetuned LLM\n",
    "print(FTOutput[12])\n",
    "print(len(FTOutput))\n",
    "print(len(FTCorrectFormat))\n",
    "sum(FTCorrectFormat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91c58ce2-55ad-45d9-9f12-0e938e96935d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. 1',\n",
       " '2. 1',\n",
       " '3. 2',\n",
       " '4. 3',\n",
       " '5. 3',\n",
       " '6. 0',\n",
       " '7. 0',\n",
       " '8. 0',\n",
       " '9. 0',\n",
       " '10. 0',\n",
       " '11. 0',\n",
       " '12. 0',\n",
       " '13. 0',\n",
       " '14. 0',\n",
       " '15. 0',\n",
       " '16. 0',\n",
       " '17. Customer reports PySpark jobs failing with garbage collection errors in CDE after increasing data processing volume from 1TB to 3TB daily. Jobs are experiencing executor losses with current configuration of 12GB executor memory and 6 cores per executor. Requesting guidance on optimal GC and memory settings.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parsed formatted output for the finetuned LLM\n",
    "Solutions[2].split('\\n')[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9c20632-9d91-42fe-9d68-8b289da67b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1, 2: 1, 3: 2, 4: 3, 5: 3, 6: 1, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: \"We're experiencing issues with our PySpark jobs in CDE where they're randomly failing with garbage collection errors after increasing our data processing volume from 1TB to 3TB daily.\"}\n",
      "500\n",
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StartLine=3\n",
    "if \"Qwen2.5\" in ModelBase:\n",
    "    StartLine=2\n",
    "if \"mistral\" in ModelBase:\n",
    "    StartLine=1\n",
    "\n",
    "if Customer==1:\n",
    "  (BaselineOutput,BaselineCorrectFormat)=FormatOutput(SolutionsBase,StartLine=StartLine)\n",
    "elif Customer==0:\n",
    "  (BaselineOutput,BaselineCorrectFormat)=FormatOutput(SolutionsBase,StartLine=StartLine,ExpectedQuestions=11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d7b136-f25b-4bc8-a34c-c3090e4bd478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsed formatted output for the base LLM\n",
    "\n",
    "print(BaselineOutput[2])\n",
    "print(len(BaselineOutput))\n",
    "print(len(BaselineCorrectFormat))\n",
    "sum(BaselineCorrectFormat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8820c54e-4c32-4531-a1b4-7868c8ba51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompts and 3 examples for LLM-as-a-judge to score whether model A or B wins or if there is a tie.\n",
    "\n",
    "\n",
    "comment=\"Hi Support team, we're experiencing critical performance issues with our Hive queries in our production environment. All queries are taking 3-4 times longer than usual to execute, severely impacting our business operations. Can you please escalate this to a high priority case and have someone look into this ASAP? We need immediate assistance as this is affecting our SLAs.\"\n",
    "question=\"\"\"Does this comment relate to a customer complaint? (BOOL)\"\"\"\n",
    "AnswerA=\"\"\"0\"\"\"\n",
    "AnswerB=\"\"\"1\"\"\"\n",
    "\n",
    "Example1Prompt=\"You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \\n\" + comment +\".\\nHere is the question:\\n\" + question+\":\\n\" +\"\\nHere is the answer of System A:\\n\"+ AnswerA +\"\\nHere is the answer of System B:\\n\"+AnswerB\n",
    "\n",
    "Example1Completion=\"\"\"```json\n",
    "{\n",
    "    \"choice\": \"B\"\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "comment=\"We're experiencing issues with our Cloudera Data Warehouse environment where our Impala queries are failing with OOM errors. The cluster metrics show high memory utilization across all nodes. This is blocking our critical reporting pipeline. Could you please help investigate this issue? We need this resolved within the next 24 hours.\"\n",
    "question=\"\"\"Customer complaint temperature or a frustration level (score 1-4, 0 if not a complaint)\"\"\"\n",
    "AnswerA=\"\"\"3\"\"\"\n",
    "AnswerB=\"\"\"3\"\"\"\n",
    "\n",
    "Example2Prompt=\"You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \\n\" + comment +\".\\nHere is the question:\\n\" + question+\":\\n\" +\"\\nHere is the answer of System A:\\n\"+ AnswerA +\"\\nHere is the answer of System B:\\n\"+AnswerB\n",
    "\n",
    "Example2Completion=\"\"\"```json\n",
    "{\n",
    "    \"choice\": \"tie\"\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "comment=\"Our Cloudera Manager is showing multiple critical alerts for HDFS data nodes. The replication factor has dropped below the minimum threshold, and we're seeing increased latency in our data processing jobs. Can you please help us resolve this urgently? This is impacting our production workloads.\"\n",
    "question=\"\"\"Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]\"\"\"\n",
    "AnswerA=\"\"\"Critical HDFS issues with data nodes showing alerts, reduced replication factor causing increased latency in production data processing jobs.\"\"\"\n",
    "AnswerB=\"\"\"This is about critical alets.\"\"\"\n",
    "\n",
    "Example3Prompt=\"You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \\n\" + comment +\".\\nHere is the question:\\n\" + question+\":\\n\" +\"\\nHere is the answer of System A:\\n\"+ AnswerA +\"\\nHere is the answer of System B:\\n\"+AnswerB\n",
    "Example3Completion=\"\"\"```json\n",
    "{\n",
    "    \"choice\": \"A\"\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09e765f0-b4dc-4016-93ca-36db519bd5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat2 = [\n",
    "  {\"role\": \"system\", \"content\": System},\n",
    "\n",
    "  {\"role\": \"user\", \"content\": Example1Prompt},\n",
    "  {\"role\": \"assistant\", \"content\": Example1Completion},\n",
    "  {\"role\": \"user\", \"content\": Example2Prompt},\n",
    "  {\"role\": \"assistant\", \"content\": Example2Completion},\n",
    "  {\"role\": \"user\", \"content\": Example3Prompt},\n",
    "  {\"role\": \"assistant\", \"content\": Example3Completion}\n",
    "]\n",
    "\n",
    "Completion=\"\"\"```json\n",
    "{\n",
    "    \"choice\": \"\"\"\n",
    "\n",
    "chat=chat2.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6c95da6-8f65-4251-8810-e4c18d3e2adc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "[100264, 9125, 100266, 2675, 527, 264, 11190, 18328, 13, 100265, 100264, 882, 100266, 2675, 527, 2728, 264, 4068, 323, 264, 3488, 922, 279, 4068, 13, 1472, 690, 387, 2728, 279, 2077, 315, 1403, 6067, 323, 499, 1205, 311, 11913, 902, 1887, 220, 6688, 279, 1888, 2077, 13, 7557, 2771, 499, 3493, 279, 4320, 304, 3024, 323, 701, 5873, 374, 2728, 304, 264, 2115, 449, 279, 907, 5873, 13, 5810, 374, 279, 4068, 25, 720, 13347, 9365, 2128, 11, 584, 2351, 25051, 9200, 5178, 4819, 449, 1057, 69278, 20126, 304, 1057, 5788, 4676, 13, 2052, 20126, 527, 4737, 220, 18, 12, 19, 3115, 5129, 1109, 13783, 311, 9203, 11, 35906, 74055, 1057, 2626, 7677, 13, 3053, 499, 4587, 89690, 420, 311, 264, 1579, 10844, 1162, 323, 617, 4423, 1427, 1139, 420, 67590, 30, 1226, 1205, 14247, 13291, 439, 420, 374, 28987, 1057, 17216, 2170, 35047, 8586, 374, 279, 3488, 512, 22186, 420, 4068, 29243, 311, 264, 6130, 12458, 30, 320, 10611, 7887, 8586, 374, 279, 4320, 315, 744, 362, 512, 15, 198, 8586, 374, 279, 4320, 315, 744, 426, 512, 16, 100265, 100264, 78191, 100266, 74694, 2285, 198, 517, 262, 330, 12008, 794, 330, 33, 702, 534, 74694, 100265, 100264, 882, 100266, 2675, 527, 2728, 264, 4068, 323, 264, 3488, 922, 279, 4068, 13, 1472, 690, 387, 2728, 279, 2077, 315, 1403, 6067, 323, 499, 1205, 311, 11913, 902, 1887, 220, 6688, 279, 1888, 2077, 13, 7557, 2771, 499, 3493, 279, 4320, 304, 3024, 323, 701, 5873, 374, 2728, 304, 264, 2115, 449, 279, 907, 5873, 13, 5810, 374, 279, 4068, 25, 720, 1687, 2351, 25051, 4819, 449, 1057, 2493, 283, 99003, 2956, 52466, 4676, 1405, 1057, 14727, 6181, 20126, 527, 22109, 449, 507, 1937, 6103, 13, 578, 10879, 17150, 1501, 1579, 5044, 50549, 4028, 682, 7954, 13, 1115, 374, 22978, 1057, 9200, 13122, 15660, 13, 16910, 499, 4587, 1520, 19874, 420, 4360, 30, 1226, 1205, 420, 20250, 2949, 279, 1828, 220, 1187, 4207, 35047, 8586, 374, 279, 3488, 512, 13084, 12458, 9499, 477, 264, 33086, 2237, 320, 12618, 220, 16, 12, 19, 11, 220, 15, 422, 539, 264, 12458, 7887, 8586, 374, 279, 4320, 315, 744, 362, 512, 18, 198, 8586, 374, 279, 4320, 315, 744, 426, 512, 18, 100265, 100264, 78191, 100266, 74694, 2285, 198, 517, 262, 330, 12008, 794, 330, 49831, 702, 534, 74694, 100265, 100264, 882, 100266, 2675, 527, 2728, 264, 4068, 323, 264, 3488, 922, 279, 4068, 13, 1472, 690, 387, 2728, 279, 2077, 315, 1403, 6067, 323, 499, 1205, 311, 11913, 902, 1887, 220, 6688, 279, 1888, 2077, 13, 7557, 2771, 499, 3493, 279, 4320, 304, 3024, 323, 701, 5873, 374, 2728, 304, 264, 2115, 449, 279, 907, 5873, 13, 5810, 374, 279, 4068, 25, 720, 8140, 2493, 283, 99003, 10790, 374, 9204, 5361, 9200, 30350, 369, 473, 63366, 828, 7954, 13, 578, 48891, 8331, 706, 12504, 3770, 279, 8187, 12447, 11, 323, 584, 2351, 9298, 7319, 40370, 304, 1057, 828, 8863, 7032, 13, 3053, 499, 4587, 1520, 603, 9006, 420, 77720, 30, 1115, 374, 74055, 1057, 5788, 990, 33785, 35047, 8586, 374, 279, 3488, 512, 9370, 5730, 553, 279, 1162, 4068, 9955, 49205, 433, 439, 1790, 439, 3284, 719, 2085, 13490, 3062, 11156, 3649, 13, 507, 1800, 2737, 904, 6574, 22114, 2038, 13, 220, 510, 12998, 69662, 8586, 374, 279, 4320, 315, 744, 362, 512, 43108, 473, 63366, 4819, 449, 828, 7954, 9204, 30350, 11, 11293, 48891, 8331, 14718, 7319, 40370, 304, 5788, 828, 8863, 7032, 627, 8586, 374, 279, 4320, 315, 744, 426, 512, 2028, 374, 922, 9200, 264, 10145, 13, 100265, 100264, 78191, 100266, 74694, 2285, 198, 517, 262, 330, 12008, 794, 330, 32, 702, 534, 74694, 100265, 100264, 882, 100266, 2675, 527, 2728, 264, 4068, 323, 264, 3488, 922, 279, 4068, 13, 1472, 690, 387, 2728, 279, 2077, 315, 1403, 6067, 11, 1887, 362, 323, 1887, 426, 323, 499, 1205, 311, 11913, 902, 1887, 220, 6688, 279, 1888, 2077, 477, 422, 433, 574, 264, 18623, 13, 7557, 2771, 499, 3493, 279, 4320, 304, 3024, 323, 701, 5873, 374, 2728, 304, 264, 2115, 449, 279, 907, 5873, 13, 5810, 374, 279, 4068, 25, 720, 13347, 2493, 283, 99003, 9365, 3638, 1687, 2351, 3515, 4819, 449, 25976, 264, 502, 5780, 6975, 15660, 430, 75762, 35219, 596, 73235, 8471, 449, 1057, 356, 2735, 4676, 13, 578, 1646, 24047, 374, 22109, 449, 17066, 6103, 11, 323, 584, 2351, 25051, 1579, 40370, 2391, 5446, 6880, 13, 23212, 11, 279, 2587, 17150, 584, 3077, 743, 709, 7784, 956, 1694, 14042, 10489, 13, 16910, 499, 1520, 603, 20498, 97508, 1521, 18052, 4819, 1980, 14809, 24886, 345, 72526, 627, 8586, 374, 279, 3488, 512, 9370, 5730, 553, 279, 1162, 4068, 9955, 49205, 433, 439, 1790, 439, 3284, 719, 2085, 13490, 3062, 11156, 3649, 13, 507, 1800, 2737, 904, 6574, 22114, 2038, 13, 220, 510, 12998, 933, 8586, 374, 279, 4320, 315, 744, 362, 512, 1687, 2351, 25051, 4819, 449, 25976, 264, 502, 5780, 6975, 15660, 430, 75762, 35219, 596, 73235, 8471, 449, 1057, 356, 2735, 4676, 13, 578, 1646, 24047, 374, 22109, 449, 17066, 6103, 11, 323, 584, 2351, 25051, 1579, 40370, 2391, 5446, 6880, 11, 323, 2587, 17150, 7784, 956, 1694, 14042, 10489, 627, 8586, 374, 279, 4320, 315, 744, 426, 512, 13084, 6821, 4819, 449, 35219, 73235, 8471, 18052, 304, 356, 2735, 4676, 11, 2737, 17066, 6103, 11, 1579, 40370, 2391, 5446, 6880, 11, 323, 41086, 2587, 18767, 8558, 2391, 1646, 24047, 13, 100265, 100264, 78191, 100266, 74694, 2285, 198, 517, 262, 330, 12008, 794]\n"
     ]
    }
   ],
   "source": [
    "#Setup LLM-as-a-judge final prompt\n",
    "from transformers import AutoTokenizer\n",
    "EvalLLM='microsoft/phi-4'\n",
    "RemoveAssistant=2\n",
    "if EvalLLM=='microsoft/phi-4':\n",
    "    RemoveAssistant=1\n",
    "elif EvalLLM=='Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8':\n",
    "    RemoveAssistant=2\n",
    "#tokenizer = AutoTokenizer.from_pretrained(EvalLLM)\n",
    "tokenizer = AutoTokenizer.from_pretrained(EvalLLM)\n",
    "\n",
    "if Customer==1:   \n",
    "    ExpectedQuestions=17\n",
    "\n",
    "elif Customer==0:\n",
    "    ExpectedQuestions=11\n",
    "    \n",
    "FTCorrectFormat={}\n",
    "BaselineCorrectFormat={}\n",
    "AllPrompts=[]\n",
    "\n",
    "for i in range(1,ExpectedQuestions+1):\n",
    "    FTCorrectFormat[i]={}\n",
    "    BaselineCorrectFormat[i]={}\n",
    "    Prompts=[]\n",
    "    print(i)\n",
    "    for j in range(0,len(FTOutput)):\n",
    "        chat=chat2.copy()\n",
    "        \n",
    "        if i in FTOutput[j]:\n",
    "            FTCandidate=str(FTOutput[j][i])\n",
    "            FTCorrectFormat[i][j]=True\n",
    "        else:\n",
    "            FTCandidate=\"\"\n",
    "            FTCorrectFormat[i][j]=False\n",
    "            \n",
    "        if i in BaselineOutput[j]:\n",
    "            BaselineCandidate=str(BaselineOutput[j][i])\n",
    "            BaselineCorrectFormat[i][j]=True\n",
    "        else:\n",
    "            BaselineCandidate=\"\"\n",
    "            BaselineCorrectFormat[i][j]=False\n",
    "        user=\"\"\n",
    "        if j%2==0:\n",
    "          user=\"You are given a comment and a question about the comment. You will be given the response of two systems, system A and system B and you need to judge which system  gave the best response or if it was a tie. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \\n\" + Comment[j]+\".\\nHere is the question:\\n\"+FixedQuestions[i]+\"\\nHere is the answer of System A:\\n\" + FTCandidate +\"\\nHere is the answer of System B:\\n\"+BaselineCandidate + \"\\n\"\n",
    "        else:\n",
    "          user=\"You are given a comment and a question about the comment. You will be given the response of two systems, system A and system B and you need to judge which system  gave the best response or if it was a tie. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \\n\" + Comment[j]+\".\\nHere is the question:\\n\"+FixedQuestions[i]+\"\\nHere is the answer of System A:\\n\" + BaselineCandidate +\"\\nHere is the answer of System B:\\n\"+FTCandidate + \"\\n\"\n",
    "\n",
    "        chat.append({\"role\": \"user\", \"content\": user})\n",
    "        chat.append({\"role\": \"assistant\", \"content\": Completion})\n",
    "        \n",
    "        Prompt=tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "        Prompt=tokenizer.decode(tokenizer.encode(Prompt)[:-RemoveAssistant])\n",
    "        Prompts.append(Prompt)\n",
    "    AllPrompts.append(Prompts)\n",
    "    \n",
    "\n",
    "\n",
    "print(tokenizer.encode(Prompts[1]))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ac083d5-82d9-460f-8d64-6341b2303901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system<|im_sep|>You are a helpful assistant.<|im_end|><|im_start|>user<|im_sep|>You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \n",
      "Hi Support team, we're experiencing critical performance issues with our Hive queries in our production environment. All queries are taking 3-4 times longer than usual to execute, severely impacting our business operations. Can you please escalate this to a high priority case and have someone look into this ASAP? We need immediate assistance as this is affecting our SLAs..\n",
      "Here is the question:\n",
      "Does this comment relate to a customer complaint? (BOOL):\n",
      "\n",
      "Here is the answer of System A:\n",
      "0\n",
      "Here is the answer of System B:\n",
      "1<|im_end|><|im_start|>assistant<|im_sep|>```json\n",
      "{\n",
      "    \"choice\": \"B\"\n",
      "}\n",
      "```<|im_end|><|im_start|>user<|im_sep|>You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \n",
      "We're experiencing issues with our Cloudera Data Warehouse environment where our Impala queries are failing with OOM errors. The cluster metrics show high memory utilization across all nodes. This is blocking our critical reporting pipeline. Could you please help investigate this issue? We need this resolved within the next 24 hours..\n",
      "Here is the question:\n",
      "Customer complaint temperature or a frustration level (score 1-4, 0 if not a complaint):\n",
      "\n",
      "Here is the answer of System A:\n",
      "3\n",
      "Here is the answer of System B:\n",
      "3<|im_end|><|im_start|>assistant<|im_sep|>```json\n",
      "{\n",
      "    \"choice\": \"tie\"\n",
      "}\n",
      "```<|im_end|><|im_start|>user<|im_sep|>You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \n",
      "Our Cloudera Manager is showing multiple critical alerts for HDFS data nodes. The replication factor has dropped below the minimum threshold, and we're seeing increased latency in our data processing jobs. Can you please help us resolve this urgently? This is impacting our production workloads..\n",
      "Here is the question:\n",
      "Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]:\n",
      "\n",
      "Here is the answer of System A:\n",
      "Critical HDFS issues with data nodes showing alerts, reduced replication factor causing increased latency in production data processing jobs.\n",
      "Here is the answer of System B:\n",
      "This is about critical alets.<|im_end|><|im_start|>assistant<|im_sep|>```json\n",
      "{\n",
      "    \"choice\": \"A\"\n",
      "}\n",
      "```<|im_end|><|im_start|>user<|im_sep|>You are given a comment and a question about the comment. You will be given the response of two systems, system A and system B and you need to judge which system  gave the best response or if it was a tie. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \n",
      "Hi Cloudera Support, we're experiencing delays in our order tracking system after attempting to merge tracking data from multiple carriers. The ETL job that consolidates tracking information has been failing since yesterday, resulting in missing tracking updates for approximately 1,500 packages. Our customers can't see their package locations and estimated delivery times. This is affecting our ability to manage customer expectations effectively..\n",
      "Here is the question:\n",
      "Does this comment discuss any technical information? (BOOL: 0 for no, 1 for yes)\n",
      "Here is the answer of System A:\n",
      "1\n",
      "Here is the answer of System B:\n",
      "1<|im_end|><|im_start|>assistant<|im_sep|>```json\n",
      "{\n",
      "    \"choice\":\n"
     ]
    }
   ],
   "source": [
    "#LLM-as-a-judge prompt example\n",
    "print(AllPrompts[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c621694-d7ae-4825-a63a-04fc9abed761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-21 19:25:00 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 03-21 19:25:00 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='microsoft/phi-4', speculative_config=None, tokenizer='microsoft/phi-4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/phi-4, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 03-21 19:25:01 model_runner.py:1060] Starting to load model microsoft/phi-4...\n",
      "INFO 03-21 19:25:02 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:00<00:03,  1.40it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:01<00:02,  1.34it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:02<00:02,  1.34it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:02<00:01,  1.36it/s]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:03<00:00,  1.28it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:04<00:00,  1.31it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:04<00:00,  1.32it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-21 19:25:07 model_runner.py:1071] Loading model weights took 27.3875 GB\n",
      "INFO 03-21 19:25:09 gpu_executor.py:122] # GPU blocks: 702, # CPU blocks: 1310\n",
      "INFO 03-21 19:25:09 gpu_executor.py:126] Maximum concurrency for 10000 tokens per request: 1.12x\n"
     ]
    }
   ],
   "source": [
    "#Load LLM-as-a-judge\n",
    "max_num_seqs=1\n",
    "max_model_len=10000\n",
    "max_num_batched_tokens=10000 \n",
    "gpu_memory_utilization=0.7\n",
    "enforce_eager=True\n",
    "tensor_parallel_size=1\n",
    "seed=None\n",
    "max_tokens=10\n",
    "temperature=0.1\n",
    "\n",
    "import vllm\n",
    "llm = vllm.LLM(model=EvalLLM, max_num_seqs=max_num_seqs,max_model_len=max_model_len,max_num_batched_tokens=max_num_batched_tokens, gpu_memory_utilization=gpu_memory_utilization,enforce_eager=enforce_eager, tensor_parallel_size=tensor_parallel_size)\n",
    "seed=None\n",
    "max_tokens=max_tokens\n",
    "temperature=temperature\n",
    "sampling_params = vllm.SamplingParams(seed=None,max_tokens=max_tokens,temperature=temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffb2b0ac-8329-4db3-94a3-eb72e5c76a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:48<00:00,  2.97it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 1\n",
      "===================\n",
      "51.06382978723404%\n",
      "48.93617021276596%\n",
      "0.048\n",
      "0.046\n",
      "24\n",
      "23\n",
      "453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:47<00:00,  2.98it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 2\n",
      "===================\n",
      "25.0%\n",
      "75.0%\n",
      "0.016\n",
      "0.048\n",
      "8\n",
      "24\n",
      "468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:52<00:00,  2.90it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 3\n",
      "===================\n",
      "14.975845410628018%\n",
      "85.02415458937197%\n",
      "0.062\n",
      "0.352\n",
      "31\n",
      "176\n",
      "293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [03:21<00:00,  2.48it/s, est. speed input: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 4\n",
      "===================\n",
      "8.304498269896193%\n",
      "91.6955017301038%\n",
      "0.048\n",
      "0.53\n",
      "24\n",
      "265\n",
      "211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [03:08<00:00,  2.66it/s, est. speed input: 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 5\n",
      "===================\n",
      "10.104529616724738%\n",
      "89.89547038327527%\n",
      "0.058\n",
      "0.516\n",
      "29\n",
      "258\n",
      "213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:47<00:00,  2.98it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 6\n",
      "===================\n",
      "29.155313351498634%\n",
      "70.84468664850137%\n",
      "0.214\n",
      "0.52\n",
      "107\n",
      "260\n",
      "133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:48<00:00,  2.96it/s, est. speed input: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 7\n",
      "===================\n",
      "0.0%\n",
      "100.0%\n",
      "0.0\n",
      "0.036\n",
      "0\n",
      "18\n",
      "482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:47<00:00,  2.98it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 8\n",
      "===================\n",
      "40.0%\n",
      "60.0%\n",
      "0.012\n",
      "0.018\n",
      "6\n",
      "9\n",
      "485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:49<00:00,  2.94it/s, est. speed input: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 9\n",
      "===================\n",
      "23.809523809523807%\n",
      "76.19047619047619%\n",
      "0.01\n",
      "0.032\n",
      "5\n",
      "16\n",
      "479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:48<00:00,  2.97it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 10\n",
      "===================\n",
      "20.0%\n",
      "80.0%\n",
      "0.006\n",
      "0.024\n",
      "3\n",
      "12\n",
      "485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:47<00:00,  2.98it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 11\n",
      "===================\n",
      "13.333333333333334%\n",
      "86.66666666666667%\n",
      "0.004\n",
      "0.026\n",
      "2\n",
      "13\n",
      "485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:48<00:00,  2.96it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 12\n",
      "===================\n",
      "40.0%\n",
      "60.0%\n",
      "0.012\n",
      "0.018\n",
      "6\n",
      "9\n",
      "485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:48<00:00,  2.96it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 13\n",
      "===================\n",
      "50.877192982456144%\n",
      "49.122807017543856%\n",
      "0.058\n",
      "0.056\n",
      "29\n",
      "28\n",
      "443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:48<00:00,  2.96it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 14\n",
      "===================\n",
      "0.0%\n",
      "100.0%\n",
      "0.0\n",
      "0.03\n",
      "0\n",
      "15\n",
      "485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:49<00:00,  2.94it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 15\n",
      "===================\n",
      "37.5%\n",
      "62.5%\n",
      "0.012\n",
      "0.02\n",
      "6\n",
      "10\n",
      "484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:48<00:00,  2.96it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 16\n",
      "===================\n",
      "33.33333333333333%\n",
      "66.66666666666666%\n",
      "0.01\n",
      "0.02\n",
      "5\n",
      "10\n",
      "485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:55<00:00,  2.85it/s, est. speed input: 25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 17\n",
      "===================\n",
      "89.0909090909091%\n",
      "10.909090909090908%\n",
      "0.49\n",
      "0.06\n",
      "245\n",
      "30\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Run LLM-as-a-judge for eacb question and at the same time count correct answers/ties.\n",
    "Outs=[]\n",
    "PosPercNoTies=[]\n",
    "NegPercNoTies=[]\n",
    "PosPercTies=[]\n",
    "NegPercTies=[]\n",
    "countPosArr=[]\n",
    "countNegArr=[]\n",
    "countTieArr=[]\n",
    "TiesPerc=[]\n",
    "\n",
    "for j in range(0,ExpectedQuestions):\n",
    "  max_num_seqs=16\n",
    "\n",
    "  sampling_params = vllm.SamplingParams(seed=1,max_tokens=max_tokens,temperature=temperature)\n",
    "\n",
    "  Out=llm.generate(AllPrompts[j], sampling_params)\n",
    "  Outs.append(Out)\n",
    "  countPos=0\n",
    "  countNeg=0\n",
    "  countTie=0\n",
    "\n",
    "  for i in range(0,len(Out)):\n",
    "    if i%2==0:\n",
    "      #print(i)\n",
    "      #print(Out[i].prompt)\n",
    "      #print(Out[i].outputs[0].text[2])\n",
    "      if len(Out[i].outputs[0].text)<2:\n",
    "        continue\n",
    "\n",
    "      if Out[i].outputs[0].text[2]=='A':\n",
    "        countPos+=1\n",
    "      elif Out[i].outputs[0].text[2]=='B':\n",
    "        countNeg+=1\n",
    "      elif Out[i].outputs[0].text[2]=='t':\n",
    "        countTie+=1\n",
    "\n",
    "    if i%2==1:\n",
    "      if len(Out[i].outputs[0].text)<2:\n",
    "        continue\n",
    "\n",
    "      if Out[i].outputs[0].text[2]=='B':\n",
    "        countPos+=1\n",
    "      elif Out[i].outputs[0].text[2]=='A':\n",
    "        countNeg+=1\n",
    "      elif Out[i].outputs[0].text[2]=='t':\n",
    "        countTie+=1\n",
    "  print(\"===================\")\n",
    "  print(\"Question \"+str(j+1))\n",
    "  print(\"===================\")\n",
    "  print(str(countPos/(countPos+countNeg)*100)+\"%\")\n",
    "  print(str(countNeg/(countPos+countNeg)*100)+\"%\")\n",
    "  print(countPos/(countPos+countNeg+countTie))\n",
    "  print(countNeg/(countPos+countNeg+countTie))\n",
    "\n",
    "  print(countPos)\n",
    "  print(countNeg)\n",
    "  print(countTie)\n",
    "  PosPercNoTies.append(str(countPos/(countPos+countNeg)*100)+\"%\")\n",
    "  NegPercNoTies.append(str(countNeg/(countPos+countNeg)*100)+\"%\")\n",
    "  TiesPerc.append(countTie/(countPos+countNeg+countTie))\n",
    "  PosPercTies.append(countPos/(countPos+countNeg+countTie))\n",
    "  NegPercTies.append(countNeg/(countPos+countNeg+countTie))\n",
    "  countPosArr.append(countPos)\n",
    "  countNegArr.append(countNeg)\n",
    "  countTieArr.append(countTie)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6efe9958-6af1-4e88-a15e-36751c90d739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.06382978723404%\n",
      "25.0%\n",
      "14.975845410628018%\n",
      "8.304498269896193%\n",
      "10.104529616724738%\n",
      "29.155313351498634%\n",
      "0.0%\n",
      "40.0%\n",
      "23.809523809523807%\n",
      "20.0%\n",
      "13.333333333333334%\n",
      "40.0%\n",
      "50.877192982456144%\n",
      "0.0%\n",
      "37.5%\n",
      "33.33333333333333%\n",
      "89.0909090909091%\n"
     ]
    }
   ],
   "source": [
    "#Compute winrate\n",
    "for i in PosPercNoTies:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93135d09-9cc9-48a0-942a-2a0136e93d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.93617021276596%\n",
      "75.0%\n",
      "85.02415458937197%\n",
      "91.6955017301038%\n",
      "89.89547038327527%\n",
      "70.84468664850137%\n",
      "100.0%\n",
      "60.0%\n",
      "76.19047619047619%\n",
      "80.0%\n",
      "86.66666666666667%\n",
      "60.0%\n",
      "49.122807017543856%\n",
      "100.0%\n",
      "62.5%\n",
      "66.66666666666666%\n",
      "10.909090909090908%\n"
     ]
    }
   ],
   "source": [
    "#Compute lose rate!\n",
    "for i in NegPercNoTies:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54cfbd2a-6931-4f78-8e62-45201e5b2174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.906\n",
      "0.936\n",
      "0.586\n",
      "0.422\n",
      "0.426\n",
      "0.266\n",
      "0.964\n",
      "0.97\n",
      "0.958\n",
      "0.97\n",
      "0.97\n",
      "0.97\n",
      "0.886\n",
      "0.97\n",
      "0.968\n",
      "0.97\n",
      "0.45\n"
     ]
    }
   ],
   "source": [
    "#Compute ties.\n",
    "for i in TiesPerc:\n",
    "    print(str(i*100) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
