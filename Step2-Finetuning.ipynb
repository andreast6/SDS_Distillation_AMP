{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f51704f-f8b5-46eb-a852-e3048b5759db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-21 21:00:05,672\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    }
   ],
   "source": [
    "#variables setup for fineuting\n",
    "import pickle \n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import Libs.DataSynthesisLib as DataSynthesisLib\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "from sys import exit\n",
    "from multiprocess import set_start_method\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "from peft import PeftModel\n",
    "import torch.multiprocessing  as mp\n",
    "\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "GPU_ID=\"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=GPU_ID\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "\n",
    "Config=DataSynthesisLib.GetConfig('Config/Config.py')\n",
    "Vendor=\"unsloth\"\n",
    "BaseModel=\"Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "\n",
    "QuantModel=BaseModel+\"\"\n",
    "UnQuantModel=BaseModel\n",
    "\n",
    "#Output model path\n",
    "TargetDir=\"/tmp/\"\n",
    "TargetDirOutModel=\"/tmp/\"\n",
    "\n",
    "UnQuantModelVendor=Vendor+\"/\"+UnQuantModel\n",
    "\n",
    "ModelFT=Vendor+\"/\"+UnQuantModel\n",
    "FilenameShort='AllComments_Clean_Train'\n",
    "Filename='Data/AllComments_Clean_Train'\n",
    "\n",
    "with open(Filename + \".json\", \"r\") as f:\n",
    "    TrainData=json.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6df2593-433f-4fc6-aa29-3589d550c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finetuning parameters\n",
    "Data_Size=5000\n",
    "#########CHange per model\n",
    "AssistantToken=\"<|start_header_id|>assistant\"\n",
    "TrimBeg=1\n",
    "Filename2=Filename+\"_Datasize_\"+str(Data_Size) +'_'+BaseModel\n",
    "ConfigS=Config[\"ConfigSolution\"].copy()\n",
    "ConfigS[\"lora_r\"]=128\n",
    "ConfigS[\"target_modules\"]=['q_proj','k_proj','v_proj','o_proj']\n",
    "ConfigS[\"target_modules\"]=\"all-linear\"\n",
    "ConfigS[\"lora_alpha\"]=64\n",
    "ConfigS[\"lora_dropout\"]=0.05\n",
    "\n",
    "ConfigS[\"gpu-id\"]=GPU_ID\n",
    "\n",
    "ConfigS[\"max_model_len\"]=\"8000\"\n",
    "ConfigS[\"max_num_batched_tokens\"]=\"8000\"\n",
    "ConfigS[\"llm-path\"]=ModelFT\n",
    "ConfigS[\"tokenizer-path\"]=ModelFT\n",
    "ConfigS[\"llm-path\"]=ModelFT\n",
    "ConfigS[\"tokenizer-path\"]=ModelFT\n",
    "\n",
    "ConfigS[\"tokenizer-file-gguf\"]=\"\"\n",
    "ConfigS[\"tensor_parallel_size\"]=\"1\"\n",
    "ConfigS[\"GenerationMode\"]=\"InferenceProcessBlocked\"\n",
    "ConfigS[\"use-grammar\"]=\"False\"\n",
    "ConfigS[\"BlockSize\"]=\"400\"\n",
    "ConfigS[\"FT-num_train_epochs\"]=\"1\"\n",
    "\n",
    "ConfigS[\"AssistantPrompt\"]=\"Sure, here is the python solution:\\n```python\"\n",
    "ConfigS[\"GenerateChatTemplate\"]=\"GenerateChatWithSystemWithAssistant\"\n",
    "ConfigS[\"UseOutlines\"]=\"False\"\n",
    "ConfigS[\"AssistantPromptElementsRemoval\"]=\"2\"\n",
    "ConfigS[\"FT-SaveModel\"]=TargetDir+Filename2\n",
    "ConfigS[\"FT-SaveModel\"]=TargetDir+\"all-linear\"+FilenameShort+\"r_\"+str(ConfigS[\"lora_r\"])+'_'+'a_'+str(ConfigS[\"lora_alpha\"])+'_'+'d_'+str(ConfigS[\"lora_dropout\"])\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ConfigS[\"gpu-id\"]\n",
    "os.environ[\"NCCL_SHM_DISABLE\"]=\"1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13d3e19d-2d0a-4fb0-80de-35da5deac91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a Cloudera support team comment, 11 questions referring to the comment, and the possible values for each question in parenthesis following the question. Here is the Cloudera comment:\n",
      "    \n",
      "Hi Support Team,\n",
      "\n",
      "Since the latest update to version 3.6.5, we're experiencing frequent data loading errors in the mobile app. The app shows 'Failed to Load Data' messages intermittently when accessing any dashboard, making it unusable for our field teams. This appears to be happening across both iOS and Android devices.\n",
      "\n",
      "We've already:\n",
      "- Updated to the latest version\n",
      "- Cleared app cache\n",
      "- Verified network connectivity\n",
      "- Tested on multiple devices\n",
      "\n",
      "This is affecting our entire field operations team of 100+ people who need access to real-time analytics.\n",
      "\n",
      "Please investigate this critical issue.\n",
      "\n",
      "Best regards,\n",
      "Andrew\n",
      "Field Operations Director\n",
      "\n",
      "Here are the 11 questions:\n",
      "1. Does this comment discuss any technical information? (answer 0 for no, 1 for yes)\n",
      "2. Score the severity of the issue based on comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "3. Score the urgency of the issue based on the comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "4. Does the comment have a proposed solution? (answer 0 for no, 1 for yes)\n",
      "5. Does the comment have a proposed workaround?  (answer 0 for no, 1 for yes)\n",
      "6. Does the comment have a request for an action from the customer?  (answer 0 for no, 1 for yes)\n",
      "7. Does this comment discuss a bug in Cloudera software? (answer 0 for no, 1 for yes)\n",
      "8. Does the comment include a non-Cloudera Apache JIRA link (e.g. a Apache JIRA link with issues.apach.org domain name)? (answer 0 for no, 1 for yes)\n",
      "9. Does the comment have a link to Cloudera Documentation or Community article? (answer 0 for no, 1 for yes)\n",
      "10. Does the comment have any other type of hyperlink? (answer 0 for no, 1 for yes)\n",
      "11. Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]    \n",
      "Generate each answer in a new line using the the format \"Number. answer\" (e.g. 1. 0)\n",
      "\n",
      "--------------------\n",
      "You are given a Cloudera support team comment, 11 questions referring to the comment, and the possible values for each question in parenthesis following the question. Here is the Cloudera comment:\n",
      "    \n",
      "Hi Support Team,\n",
      "\n",
      "We're looking to implement a real-time streaming analytics platform using bare metal servers for Cloudera. Our expected workload is around 400TB with requirements for sub-millisecond latency for critical processes. Could you help with hardware specifications and recommendations for achieving this performance target?\n",
      "\n",
      "Best regards,\n",
      "Marcus\n",
      "\n",
      "Here are the 11 questions:\n",
      "1. Does this comment discuss any technical information? (answer 0 for no, 1 for yes)\n",
      "2. Score the severity of the issue based on comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "3. Score the urgency of the issue based on the comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "4. Does the comment have a proposed solution? (answer 0 for no, 1 for yes)\n",
      "5. Does the comment have a proposed workaround?  (answer 0 for no, 1 for yes)\n",
      "6. Does the comment have a request for an action from the customer?  (answer 0 for no, 1 for yes)\n",
      "7. Does this comment discuss a bug in Cloudera software? (answer 0 for no, 1 for yes)\n",
      "8. Does the comment include a non-Cloudera Apache JIRA link (e.g. a Apache JIRA link with issues.apach.org domain name)? (answer 0 for no, 1 for yes)\n",
      "9. Does the comment have a link to Cloudera Documentation or Community article? (answer 0 for no, 1 for yes)\n",
      "10. Does the comment have any other type of hyperlink? (answer 0 for no, 1 for yes)\n",
      "11. Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]    \n",
      "Generate each answer in a new line using the the format \"Number. answer\" (e.g. 1. 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training data examples\n",
    "print(TrainData[0]['Prompt'])\n",
    "print('--------------------')\n",
    "print(TrainData[1]['Prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf8efb36-3f4c-4afa-93c3-105747531efd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert input training data to chat completion\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(ModelFT)\n",
    "\n",
    "# Helper function to format the dataset\n",
    "def chatml_format(Data,tokenizer,Data_Size):\n",
    "  Out=[]\n",
    "  OutFormat2={\n",
    "        \"text\": []\n",
    "  }\n",
    "  for i in range(0,min(len(Data['Questions']),Data_Size)):\n",
    "    #if Data['UnitTestsPass'][i]==True:\n",
    "\n",
    "      message = [{\"role\": \"system\",    \"content\": Data['system']},\n",
    "                 {\"role\": \"user\",      \"content\": Data['Questions'][i]},\n",
    "                 {\"role\": \"assistant\", \"content\": Data[\"SolutionsCleanedPython\"][i]}]\n",
    "      system_user_completion=tokenizer.apply_chat_template(message, tokenize=False,add_generation_prompt=False)\n",
    "\n",
    "      OutFormat2[\"text\"].append(system_user_completion)\n",
    "      d={}\n",
    "      d['text']=system_user_completion\n",
    "      Out.append(d)\n",
    "  return (OutFormat2,Out)\n",
    "\n",
    "MyDatasetTrain=TrainData.copy()\n",
    "\n",
    "MyDatasetTrain={}\n",
    "MyDatasetTrain[\"system\"]='You are a helpful assistant.'\n",
    "MyDatasetTrain[\"Questions\"]=[i[\"Prompt\"] for i in TrainData]\n",
    "MyDatasetTrain[\"SolutionsCleanedPython\"]=[i[\"Completion\"] for i in TrainData]\n",
    "\n",
    "\n",
    "(MyDatasetTrainSFT,MyDatasetTrainSFTJson)=chatml_format(MyDatasetTrain,tokenizer,Data_Size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee51aa12-0595-4b6e-9a36-0a06128c1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into dev/training dataset (90% training)\n",
    "\n",
    "ds_len=len(MyDatasetTrainSFT['text'])\n",
    "df_test = pd.DataFrame.from_dict(MyDatasetTrainSFT)[9*ds_len//10:]\n",
    "df_train = pd.DataFrame.from_dict(MyDatasetTrainSFT)[:9*ds_len//10]\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_test = Dataset.from_pandas(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58b10336-ddc9-4628-89ac-a0d7fdc77b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 4500\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9165477a-83a1-4c17-b919-a72162fd7814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:04<00:00,  1.22s/it]\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_8011/35425205.py:71: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "Map: 100%|█████████████████████████| 4500/4500 [00:01<00:00, 3408.64 examples/s]\n",
      "Map: 100%|███████████████████████████| 500/500 [00:00<00:00, 3480.76 examples/s]\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "No experiment set using default experiment.Please set experiment using mlflow.set_experiment('<your experiment name>') to avoid using default experiment.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='562' max='562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [562/562 25:58, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.640400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.344200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.331600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.295500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.255500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.273600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.265900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.253200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.249700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.263400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.226200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.224700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.242300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.224900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.235300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.227200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.224800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.220600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.223400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.231500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.231400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.216900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.223100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.223200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.213900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.220400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.214200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=562, training_loss=0.2668869098734601, metrics={'train_runtime': 1561.2864, 'train_samples_per_second': 2.882, 'train_steps_per_second': 0.36, 'total_flos': 1.524387104004096e+17, 'train_loss': 0.2668869098734601, 'epoch': 0.9991111111111111})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do finetuning\n",
    "\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "def SFTTrain(ConfigS,Input):\n",
    "  \n",
    "  import os\n",
    "  os.environ[\"CUDA_VISIBLE_DEVICES\"]=ConfigS[\"gpu-id\"]\n",
    "  from peft import LoraModel, LoraConfig\n",
    "  import peft\n",
    "  import torch\n",
    "  from trl import SFTConfig, SFTTrainer\n",
    "  from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "  from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorForSeq2Seq #,infer_auto_device_map\n",
    "  from peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM\n",
    "  from peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM\n",
    "  device_map={'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 0, 'model.layers.21': 0, 'model.layers.22': 0, 'model.layers.23': 0, 'model.layers.24': 0, 'model.layers.25': 0, 'model.layers.26': 0, 'model.layers.27': 0, 'model.layers.28': 0, 'model.layers.29': 0, 'model.layers.30': 0, 'model.layers.31': 0, 'model.layers.32': 0, 'model.layers.33': 0, 'model.layers.34': 0, 'model.layers.35': 0, 'model.layers.36': 0, 'model.layers.37': 0, 'model.layers.38': 0, 'model.layers.39': 0, 'model.layers.40': 0, 'model.layers.41': 0, 'model.layers.42': 0, 'model.layers.43': 0, 'model.layers.44': 0, 'model.layers.45': 1, 'model.layers.46': 1, 'model.layers.47': 1, 'model.layers.48': 1, 'model.layers.49': 1, 'model.layers.50': 1, 'model.layers.51': 1, 'model.layers.52': 1, 'model.layers.53': 1, 'model.layers.54': 1, 'model.layers.55': 1, 'model.layers.56': 1, 'model.layers.57': 1, 'model.layers.58': 1, 'model.layers.59': 1, 'model.layers.60': 1, 'model.layers.61': 1, 'model.layers.62': 1, 'model.layers.63': 1, 'model.norm': 1, 'model.rotary_emb': 0, 'lm_head': 1}\n",
    "  dataset_train=Input[\"dataset_train\"]\n",
    "  dataset_test=Input[\"dataset_test\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  model=AutoModelForCausalLM.from_pretrained(ConfigS[\"llm-path\"], device_map=\"cuda\",torch_dtype=torch.float16)\n",
    "  \n",
    "            \n",
    "  tokenizer = AutoTokenizer. from_pretrained(ConfigS[\"llm-path\"])\n",
    "\n",
    "  model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "\n",
    "  peft_config = LoraConfig(\n",
    "    r=ConfigS[\"lora_r\"],\n",
    "    lora_alpha=ConfigS[\"lora_alpha\"],\n",
    "    target_modules=ConfigS[\"target_modules\"],\n",
    "    lora_dropout=ConfigS[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "  )\n",
    "  model.enable_input_require_grads() \n",
    "  model = peft.get_peft_model(model, peft_config)\n",
    "  optimizer=torch.optim.AdamW(model.parameters(),float(ConfigS[\"FT-learning_rate\"]))\n",
    "  training_args = SFTConfig(   \n",
    "    max_seq_length=4096,\n",
    "    output_dir=TargetDir+ConfigS[\"FT-output-dir\"],\n",
    "    do_train=bool(ConfigS[\"FT-do_train\"]),\n",
    "    do_eval=bool(ConfigS[\"FT-do_eval\"]),\n",
    "    per_device_train_batch_size=int(ConfigS[\"FT-per_device_train_batch_size\"]),\n",
    "    per_device_eval_batch_size=int(ConfigS[\"FT-per_device_eval_batch_size\"]),\n",
    "    learning_rate=float(ConfigS[\"FT-learning_rate\"]),\n",
    "    num_train_epochs=int(ConfigS[\"FT-num_train_epochs\"]),\n",
    "    logging_dir=ConfigS[\"FT-logging_dir\"],\n",
    "    logging_steps=int(ConfigS[\"FT-logging_steps\"]),\n",
    "    save_steps=int(ConfigS[\"FT-save_steps\"]),\n",
    "    optim=ConfigS[\"FT-optim\"],\n",
    "\n",
    "    dataset_text_field='text',\n",
    "\n",
    "    gradient_accumulation_steps=int(ConfigS[\"gradient_accumulation_steps\"]),\n",
    "    eval_steps=10,\n",
    "    fp16=True\n",
    "  )\n",
    "\n",
    "  \n",
    "  trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer,None)\n",
    "  )\n",
    "  res=trainer.train()\n",
    "\n",
    "  trainer.save_model(ConfigS[\"FT-SaveModel\"])\n",
    "    \n",
    "  return res\n",
    "\n",
    "Input={}\n",
    "Input[\"dataset_train\"]=dataset_train\n",
    "Input[\"dataset_test\"]=dataset_test\n",
    "#Launch finetuning\n",
    "p=mp.Process(target=SFTTrain,args=(ConfigS,Input))\n",
    "p.start()\n",
    "p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf7e6bf9-7e2d-46fe-a6dd-08898e4f14c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsloth/Meta-Llama-3.1-8B-Instruct\n",
      "/tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05\n"
     ]
    }
   ],
   "source": [
    "TargetModel=TargetDirOutModel+'merged_'+FilenameShort+\"r_\"+str(ConfigS[\"lora_r\"])+'_'+'a_'+str(ConfigS[\"lora_alpha\"])+'_'+'d_'+str(ConfigS[\"lora_dropout\"])\n",
    "print(ModelFT)\n",
    "print(TargetModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c7514f4-8271-4c32-952f-4943e720b6e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Merge model and adapters\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tokenizer\u001b[38;5;241m=\u001b[39m\u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(ModelFT)\n\u001b[1;32m      5\u001b[0m base_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(ModelFT,torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m      7\u001b[0m peft_model_id \u001b[38;5;241m=\u001b[39m ConfigS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFT-SaveModel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#Merge model and adapters\n",
    "def MergeAndSave(ModelFT, TargetModel):\n",
    "  tokenizer=AutoTokenizer.from_pretrained(ModelFT)\n",
    "\n",
    "  base_model = AutoModelForCausalLM.from_pretrained(ModelFT,torch_dtype=torch.float16)\n",
    "\n",
    "  peft_model_id = ConfigS[\"FT-SaveModel\"]\n",
    "  model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "\n",
    "  merged_model = model.merge_and_unload()\n",
    "\n",
    "  merged_model.save_pretrained(TargetModel)\n",
    "  tokenizer.save_pretrained(TargetModel)\n",
    "\n",
    "p=mp.Process(target=MergeAndSave,args=(ModelFT, TargetModel))\n",
    "p.start()\n",
    "p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e24bf-d599-4b2c-8447-0654c79953d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
