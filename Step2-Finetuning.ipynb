{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f51704f-f8b5-46eb-a852-e3048b5759db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#variables setup for fineuting\n",
    "import pickle \n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import Libs.DataSynthesisLib as DataSynthesisLib\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "from sys import exit\n",
    "from multiprocess import set_start_method\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "from peft import PeftModel\n",
    "import torch.multiprocessing  as mp\n",
    "\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "GPU_ID=\"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=GPU_ID\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "\n",
    "Config=DataSynthesisLib.GetConfig('Config/Config.py')\n",
    "Vendor=\"unsloth\"\n",
    "BaseModel=\"Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "\n",
    "QuantModel=BaseModel+\"\"\n",
    "UnQuantModel=BaseModel\n",
    "\n",
    "#Output model path\n",
    "TargetDir=\"tmp/\"\n",
    "TargetDirOutModel=\"tmp/\"\n",
    "\n",
    "UnQuantModelVendor=Vendor+\"/\"+UnQuantModel\n",
    "\n",
    "ModelFT=Vendor+\"/\"+UnQuantModel\n",
    "FilenameShort='AllComments_Clean_Train'\n",
    "Filename='Data/AllComments_Clean_Train'\n",
    "\n",
    "with open(Filename + \".json\", \"r\") as f:\n",
    "    TrainData=json.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6df2593-433f-4fc6-aa29-3589d550c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finetuning parameters\n",
    "Data_Size=5000\n",
    "#########CHange per model\n",
    "AssistantToken=\"<|start_header_id|>assistant\"\n",
    "TrimBeg=1\n",
    "Filename2=Filename+\"_Datasize_\"+str(Data_Size) +'_'+BaseModel\n",
    "ConfigS=Config[\"ConfigSolution\"].copy()\n",
    "ConfigS[\"lora_r\"]=128\n",
    "ConfigS[\"target_modules\"]=['q_proj','k_proj','v_proj','o_proj']\n",
    "ConfigS[\"target_modules\"]=\"all-linear\"\n",
    "ConfigS[\"lora_alpha\"]=64\n",
    "ConfigS[\"lora_dropout\"]=0.05\n",
    "\n",
    "ConfigS[\"gpu-id\"]=GPU_ID\n",
    "\n",
    "ConfigS[\"max_model_len\"]=\"8000\"\n",
    "ConfigS[\"max_num_batched_tokens\"]=\"8000\"\n",
    "ConfigS[\"llm-path\"]=ModelFT\n",
    "ConfigS[\"tokenizer-path\"]=ModelFT\n",
    "ConfigS[\"llm-path\"]=ModelFT\n",
    "ConfigS[\"tokenizer-path\"]=ModelFT\n",
    "\n",
    "ConfigS[\"tokenizer-file-gguf\"]=\"\"\n",
    "ConfigS[\"tensor_parallel_size\"]=\"1\"\n",
    "ConfigS[\"GenerationMode\"]=\"InferenceProcessBlocked\"\n",
    "ConfigS[\"use-grammar\"]=\"False\"\n",
    "ConfigS[\"BlockSize\"]=\"400\"\n",
    "ConfigS[\"FT-num_train_epochs\"]=\"1\"\n",
    "\n",
    "ConfigS[\"AssistantPrompt\"]=\"Sure, here is the python solution:\\n```python\"\n",
    "ConfigS[\"GenerateChatTemplate\"]=\"GenerateChatWithSystemWithAssistant\"\n",
    "ConfigS[\"UseOutlines\"]=\"False\"\n",
    "ConfigS[\"AssistantPromptElementsRemoval\"]=\"2\"\n",
    "ConfigS[\"FT-SaveModel\"]=TargetDir+Filename2\n",
    "ConfigS[\"FT-SaveModel\"]=TargetDir+\"all-linear\"+FilenameShort+\"r_\"+str(ConfigS[\"lora_r\"])+'_'+'a_'+str(ConfigS[\"lora_alpha\"])+'_'+'d_'+str(ConfigS[\"lora_dropout\"])\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ConfigS[\"gpu-id\"]\n",
    "os.environ[\"NCCL_SHM_DISABLE\"]=\"1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13d3e19d-2d0a-4fb0-80de-35da5deac91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a Cloudera support team comment, 11 questions referring to the comment, and the possible values for each question in parenthesis following the question. Here is the Cloudera comment:\n",
      "    \n",
      "Hello Team,\n",
      "\n",
      "I am experiencing issues with setting up accessibility features in Cloudera Data Warehouse for one of our team members who has visual impairments. We need assistance with:\n",
      "\n",
      "1. Configuring screen magnification\n",
      "2. Setting up keyboard shortcuts\n",
      "3. Adjusting contrast settings\n",
      "4. Enabling voice commands\n",
      "\n",
      "Could you please provide guidance on how to properly configure these settings?\n",
      "\n",
      "Thanks,\n",
      "Mike\n",
      "\n",
      "Here are the 11 questions:\n",
      "1. Does this comment discuss any technical information? (answer 0 for no, 1 for yes)\n",
      "2. Score the severity of the issue based on comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "3. Score the urgency of the issue based on the comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "4. Does the comment have a proposed solution? (answer 0 for no, 1 for yes)\n",
      "5. Does the comment have a proposed workaround?  (answer 0 for no, 1 for yes)\n",
      "6. Does the comment have a request for an action from the customer?  (answer 0 for no, 1 for yes)\n",
      "7. Does this comment discuss a bug in Cloudera software? (answer 0 for no, 1 for yes)\n",
      "8. Does the comment include a non-Cloudera Apache JIRA link (e.g. a Apache JIRA link with issues.apach.org domain name)? (answer 0 for no, 1 for yes)\n",
      "9. Does the comment have a link to Cloudera Documentation or Community article? (answer 0 for no, 1 for yes)\n",
      "10. Does the comment have any other type of hyperlink? (answer 0 for no, 1 for yes)\n",
      "11. Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]    \n",
      "Generate each answer in a new line using the the format \"Number. answer\" (e.g. 1. 0)\n",
      "\n",
      "--------------------\n",
      "You are given a customer comment, 17 questions referring to the comment, and the possible values for each question in parenthesis following the question. Here is the customer comment:\n",
      "\n",
      "Hi Support Team,\n",
      "\n",
      "I need help finding documentation for Apache Hive performance optimization in CDP. I've been searching through the documentation portal but can't seem to locate specific guides about query optimization, partitioning strategies, and best practices for large-scale data processing. We're planning a major data warehouse upgrade next month, so having access to comprehensive documentation would be extremely helpful.\n",
      "\n",
      "Best regards,\n",
      "Alex\n",
      "\n",
      "Here are the 17 questions:\n",
      "1. Does this comment discuss any technical information? (answer 0 for no, 1 for yes)\n",
      "2. Does this comment relate to a customer complaint? (answer 0 for no, 1 for yes)\n",
      "3. Customer complaint temperature or a frustration level (if there is a complain give 1 for lowest, 4 for highest and 2,3 for in between. If there is no complain give a score of 0).\n",
      "4. Score the severity of the issue based on comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "5. Score the urgency of the issue based on the comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "6. Is this a request from a customer for an update? (answer 0 for no, 1 for yes)\n",
      "7. Is there a strictly explicit and NOT an implied request from a customer for a call, meeting or a screenshare (zoom/webex/teams etc.)? Do not answer yes unless wording explicitly asks for a call. ((BOOL:0/1)\n",
      "8. Did the customer request an escalation? (answer 0 for no, 1 for yes)\n",
      "9. Did the customer request a priority change?  To what level? (If there is a priority change give score 1 to indicate highest priority (indicated by S1) and 4  to indicate the lowest priority (Indicated by S4). If there is no priority change give a score of 0).\n",
      "10. Did the customer request a transfer to another Customer Operations Engineer? (answer 0 for no, 1 for yes)\n",
      "11. Did the customer request to speak to a manager or supervisor? (answer 0 for no, 1 for yes)\n",
      "12. Did the customer request a Subject Matter Expert or expert? (answer 0 for no, 1 for yes)\n",
      "13. Does this comment discuss a bug in Cloudera software? (answer 0 for no, 1 for yes)\n",
      "14. Does the comment include a non-Cloudera Apache JIRA link (e.g. a Apache JIRA link with issues.apach.org domain name)? (answer 0 for no, 1 for yes)\n",
      "15. Does the comment have a link to Cloudera Documentation or Community article? (answer 0 for no, 1 for yes)\n",
      "16. Does the comment have any other type of hyperlink? (answer 0 for no, 1 for yes)\n",
      "17. Summarize the case comment condensing itas much as possible but without losing important technical details. Omit including any meeting invite information. (TEXT)\n",
      "Generate each answer in a new line using the the format \"Number. answer\" (e.g. 1. 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training data examples\n",
    "print(TrainData[0]['Prompt'])\n",
    "print('--------------------')\n",
    "print(TrainData[1]['Prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf8efb36-3f4c-4afa-93c3-105747531efd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert input training data to chat completion\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(ModelFT)\n",
    "\n",
    "# Helper function to format the dataset\n",
    "def chatml_format(Data,tokenizer,Data_Size):\n",
    "  Out=[]\n",
    "  OutFormat2={\n",
    "        \"text\": []\n",
    "  }\n",
    "  for i in range(0,min(len(Data['Questions']),Data_Size)):\n",
    "    #if Data['UnitTestsPass'][i]==True:\n",
    "\n",
    "      message = [{\"role\": \"system\",    \"content\": Data['system']},\n",
    "                 {\"role\": \"user\",      \"content\": Data['Questions'][i]},\n",
    "                 {\"role\": \"assistant\", \"content\": Data[\"SolutionsCleanedPython\"][i]}]\n",
    "      system_user_completion=tokenizer.apply_chat_template(message, tokenize=False,add_generation_prompt=False)\n",
    "\n",
    "      OutFormat2[\"text\"].append(system_user_completion)\n",
    "      d={}\n",
    "      d['text']=system_user_completion\n",
    "      Out.append(d)\n",
    "  return (OutFormat2,Out)\n",
    "\n",
    "MyDatasetTrain=TrainData.copy()\n",
    "\n",
    "MyDatasetTrain={}\n",
    "MyDatasetTrain[\"system\"]='You are a helpful assistant.'\n",
    "MyDatasetTrain[\"Questions\"]=[i[\"Prompt\"] for i in TrainData]\n",
    "MyDatasetTrain[\"SolutionsCleanedPython\"]=[i[\"Completion\"] for i in TrainData]\n",
    "\n",
    "\n",
    "(MyDatasetTrainSFT,MyDatasetTrainSFTJson)=chatml_format(MyDatasetTrain,tokenizer,Data_Size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee51aa12-0595-4b6e-9a36-0a06128c1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into dev/training dataset (90% training)\n",
    "\n",
    "ds_len=len(MyDatasetTrainSFT['text'])\n",
    "df_test = pd.DataFrame.from_dict(MyDatasetTrainSFT)[9*ds_len//10:]\n",
    "df_train = pd.DataFrame.from_dict(MyDatasetTrainSFT)[:9*ds_len//10]\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_test = Dataset.from_pandas(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58b10336-ddc9-4628-89ac-a0d7fdc77b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 4500\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9165477a-83a1-4c17-b919-a72162fd7814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:04<00:00,  1.18s/it]\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_497/525853314.py:65: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "Map: 100%|█████████████████████████| 4500/4500 [00:01<00:00, 3660.39 examples/s]\n",
      "Map: 100%|███████████████████████████| 500/500 [00:00<00:00, 3767.32 examples/s]\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "No experiment set using default experiment.Please set experiment using mlflow.set_experiment('<your experiment name>') to avoid using default experiment.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='562' max='562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [562/562 25:44, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.434300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.639000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.306600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.279200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.281700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.259500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.254900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.240700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.264500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.256700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.246700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.242100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.225100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.231900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.217300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.238400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.226700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.223900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.228100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.222100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.220100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.234300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.234700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.213600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.219900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.217100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "#Do finetuning\n",
    "\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "def SFTTrain(ConfigS,Input):\n",
    "  \n",
    "  import os\n",
    "  os.environ[\"CUDA_VISIBLE_DEVICES\"]=ConfigS[\"gpu-id\"]\n",
    "  from peft import LoraModel, LoraConfig\n",
    "  import peft\n",
    "  import torch\n",
    "  from trl import SFTConfig, SFTTrainer\n",
    "  from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "  from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorForSeq2Seq #,infer_auto_device_map\n",
    "  from peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM\n",
    "  from peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM\n",
    "  device_map={'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 0, 'model.layers.21': 0, 'model.layers.22': 0, 'model.layers.23': 0, 'model.layers.24': 0, 'model.layers.25': 0, 'model.layers.26': 0, 'model.layers.27': 0, 'model.layers.28': 0, 'model.layers.29': 0, 'model.layers.30': 0, 'model.layers.31': 0, 'model.layers.32': 0, 'model.layers.33': 0, 'model.layers.34': 0, 'model.layers.35': 0, 'model.layers.36': 0, 'model.layers.37': 0, 'model.layers.38': 0, 'model.layers.39': 0, 'model.layers.40': 0, 'model.layers.41': 0, 'model.layers.42': 0, 'model.layers.43': 0, 'model.layers.44': 0, 'model.layers.45': 1, 'model.layers.46': 1, 'model.layers.47': 1, 'model.layers.48': 1, 'model.layers.49': 1, 'model.layers.50': 1, 'model.layers.51': 1, 'model.layers.52': 1, 'model.layers.53': 1, 'model.layers.54': 1, 'model.layers.55': 1, 'model.layers.56': 1, 'model.layers.57': 1, 'model.layers.58': 1, 'model.layers.59': 1, 'model.layers.60': 1, 'model.layers.61': 1, 'model.layers.62': 1, 'model.layers.63': 1, 'model.norm': 1, 'model.rotary_emb': 0, 'lm_head': 1}\n",
    "  dataset_train=Input[\"dataset_train\"]\n",
    "  dataset_test=Input[\"dataset_test\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  model=AutoModelForCausalLM.from_pretrained(ConfigS[\"llm-path\"], device_map=\"cuda\",torch_dtype=torch.float16)\n",
    "  \n",
    "            \n",
    "  tokenizer = AutoTokenizer. from_pretrained(ConfigS[\"llm-path\"])\n",
    "\n",
    "  model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "\n",
    "  peft_config = LoraConfig(\n",
    "    r=ConfigS[\"lora_r\"],\n",
    "    lora_alpha=ConfigS[\"lora_alpha\"],\n",
    "    target_modules=ConfigS[\"target_modules\"],\n",
    "    lora_dropout=ConfigS[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "  )\n",
    "  model.enable_input_require_grads() \n",
    "  model = peft.get_peft_model(model, peft_config)\n",
    "  optimizer=torch.optim.AdamW(model.parameters(),float(ConfigS[\"FT-learning_rate\"]))\n",
    "  training_args = SFTConfig(   \n",
    "    max_seq_length=4096,\n",
    "    output_dir=TargetDir+ConfigS[\"FT-output-dir\"],\n",
    "    do_train=bool(ConfigS[\"FT-do_train\"]),\n",
    "    do_eval=bool(ConfigS[\"FT-do_eval\"]),\n",
    "    per_device_train_batch_size=int(ConfigS[\"FT-per_device_train_batch_size\"]),\n",
    "    per_device_eval_batch_size=int(ConfigS[\"FT-per_device_eval_batch_size\"]),\n",
    "    learning_rate=float(ConfigS[\"FT-learning_rate\"]),\n",
    "    num_train_epochs=int(ConfigS[\"FT-num_train_epochs\"]),\n",
    "    logging_dir=ConfigS[\"FT-logging_dir\"],\n",
    "    logging_steps=int(ConfigS[\"FT-logging_steps\"]),\n",
    "    save_steps=int(ConfigS[\"FT-save_steps\"]),\n",
    "    optim=ConfigS[\"FT-optim\"],\n",
    "\n",
    "    dataset_text_field='text',\n",
    "\n",
    "    gradient_accumulation_steps=int(ConfigS[\"gradient_accumulation_steps\"]),\n",
    "    eval_steps=10,\n",
    "    fp16=True\n",
    "  )\n",
    "\n",
    "  \n",
    "  trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer,None)\n",
    "  )\n",
    "  res=trainer.train()\n",
    "\n",
    "  trainer.save_model(ConfigS[\"FT-SaveModel\"])\n",
    "    \n",
    "  return res\n",
    "\n",
    "Input={}\n",
    "Input[\"dataset_train\"]=dataset_train\n",
    "Input[\"dataset_test\"]=dataset_test\n",
    "#Launch finetuning\n",
    "p=mp.Process(target=SFTTrain,args=(ConfigS,Input))\n",
    "p.start()\n",
    "p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf7e6bf9-7e2d-46fe-a6dd-08898e4f14c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsloth/Meta-Llama-3.1-8B-Instruct\n",
      "tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05\n"
     ]
    }
   ],
   "source": [
    "TargetModel=TargetDirOutModel+'merged_'+FilenameShort+\"r_\"+str(ConfigS[\"lora_r\"])+'_'+'a_'+str(ConfigS[\"lora_alpha\"])+'_'+'d_'+str(ConfigS[\"lora_dropout\"])\n",
    "print(ModelFT)\n",
    "print(TargetModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c7514f4-8271-4c32-952f-4943e720b6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:02<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "#Merge model and adapters\n",
    "def MergeAndSave(ModelFT, TargetModel):\n",
    "  tokenizer=AutoTokenizer.from_pretrained(ModelFT)\n",
    "\n",
    "  base_model = AutoModelForCausalLM.from_pretrained(ModelFT,torch_dtype=torch.float16)\n",
    "\n",
    "  peft_model_id = ConfigS[\"FT-SaveModel\"]\n",
    "  model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "\n",
    "  merged_model = model.merge_and_unload()\n",
    "\n",
    "  merged_model.save_pretrained(TargetModel)\n",
    "  tokenizer.save_pretrained(TargetModel)\n",
    "\n",
    "p=mp.Process(target=MergeAndSave,args=(ModelFT, TargetModel))\n",
    "p.start()\n",
    "p.join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
